[{"authors":null,"categories":null,"content":" Mohan Pudasaini is a World Customs Organization (WCO) accredited techincal and operational advisor on Customs Risk Management and Data Analytics, with over a decade of experience in Customs and the public sector. He specializes in international trade and Customs matters, bringing deep knowledge and practical insight into risk-based enforcement and trade facilitation. As a WCO expert, he has conducted multiple international capacity-building missions. Currently, he serves at Nepal Customs as a Risk/Data Analyst. Mohan also has expertise in SQL, R, Python, and Power BI, enabling him to transform complex data into actionable insights. In addition to his technical expertise, Mohan is an experienced trainer who has led more than 40 capacity-building workshops. He actively contributes to international best practices in Customs administration and remains committed to advancing analytics and risk management for more efficient and resilient Customs.\nDownload my resume. ","date":1737849600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1737849600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Mohan Pudasaini is a World Customs Organization (WCO) accredited techincal and operational advisor on Customs Risk Management and Data Analytics, with over a decade of experience in Customs and the public sector.","tags":null,"title":"Mohan Kumar Pudasaini","type":"authors"},{"authors":null,"categories":null,"content":"Details of the dates and times for the programs:\rClick here Syllabus for the workshop:\rClick here\nExpected outcomes: Gain a solid understanding of Python programming Learn the basics of data analysis using Python Ability to use Python for various data analysis and visualization tasks Hands-on experience using Jupyter Notebook and relevant libraries Prepared to tackle real-world data analysis projects Develop in-demand skills in the field of data analysis Targeted group: Data Analysts/Data scientist: Currently working with different software and looking to expand their skills by learning Python for data analysis. Students: Interested in starting a career in data analysis or data science and want to learn the basics of data analysis using Python. Researchers and Scholars: Who need to analyze data for their thesis, research projects, or academic papers. Educators and Teachers: Who want to incorporate data analysis and visualization into their curriculum or learn Python as a tool for teaching data science. Business Professionals: Who need to analyze data for their work and want to learn a powerful and versatile programming language for data analysis. Course duration : 20 hours\nPrior knowledge of python: Not required\nTeaching method: Practical with live coding for example:\nUsing the operators in the table below, ask Python to solve these difficult math equations:\nAssign to a Variable, Add, Subract, Multiply, Divide, Power, Integer Divide, Remainder after Division = + - * / ** // % Exercises\nExample: What is two plus three?\n2 + 3 5 What is two times three?\nwrite code here What is two to the third power?\nwrite code here How many (whole) times does 7 go into 100?\nwrite code here ","date":1329177600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1676332800,"objectID":"780453a134ffa14495b5c0f6ff9592af","permalink":"https://pudasainimohan.com.np/courses/ds_basics/","publishdate":"2012-02-14T00:00:00Z","relpermalink":"/courses/ds_basics/","section":"courses","summary":"This 15-day workshop is designed for individuals interested in data analysis and visualization who want to learn the basics of Python programming. It covers the fundamentals of data analysis using Python and is suitable for individuals with varying levels of experience, including complete beginners.","tags":null,"title":"Basic data analysis/data science course in Python","type":"docs"},{"authors":null,"categories":null,"content":" To participate in the workshop, please click here.\nExpected outcomes: Gain a solid understanding of Python programming fundamentals. Learn the basics of data analysis using Python libraries like Pandas. Develop the ability to use Python for various data analysis and visualization tasks. Gain hands-on experience using Jupyter Notebook as your development environment. Be prepared to tackle real-world data analysis projects after building a foundational skillset. Develop in-demand skills valuable in the field of data analysis. Targeted group: Data Analysts/Data scientist: Currently working with different software and looking to expand their skills by learning Python for data analysis. Students: Interested in starting a career in data analysis or data science and want to learn the basics of data analysis using Python. Researchers and Scholars: Who need to analyze data for their thesis, research projects, or academic papers. Educators and Teachers: Who want to incorporate data analysis and visualization into their curriculum or learn Python as a tool for teaching data science.. Business Professionals: Who need to analyze data for their work and want to learn a powerful and versatile programming language for data analysis Course duration : 25 hours\nPrior knowledge of python: Not required\nTeaching method: Theoretical and hands-on sessions\nUsing the operators in the table below, ask Python to solve these difficult math equations:\nAssign to a Variable, Add, Subract, Multiply, Divide, Power, Integer Divide, Remainder after Division = + - * / ** // % Exercises\nExample: What is two plus three?\n2 + 3 5 What is two times three?\nwrite code here What is two to the third power?\nwrite code here How many (whole) times does 7 go into 100?\nwrite code here ","date":1329177600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1709942400,"objectID":"8382fd82718e934dad1ca1ad9e466d9b","permalink":"https://pudasainimohan.com.np/courses/python_ml/","publishdate":"2012-02-14T00:00:00Z","relpermalink":"/courses/python_ml/","section":"courses","summary":"This 25 hours immersive workshop equips  with the essential tools to tackle real-world challenges.  With no prior Python experience needed, you'll build a strong foundation in Python programming, explore powerful data manipulation techniques with Pandas, and craft impactful data visualizations using Matplotlib. We'll then introduce you to the exciting world of Machine Learning and Large Language Models (LLMs), sparking your curiosity in these cutting-edge fields.","tags":null,"title":"Data analysis and machine learning course in Python","type":"docs"},{"authors":null,"categories":null,"content":" To participate in the workshop, please click here.\nExpected outcomes: Gain a solid understanding of R programming fundamentals. Learn the basics of data analysis using R libraries like dplyr and ggplot2. Develop the ability to use R for various data analysis and visualization tasks. Gain hands-on experience using RStudio as your development environment. Be prepared to tackle real-world data analysis projects after building a foundational skillset. Develop in-demand skills valuable in the field of data analysis. Complete data analysis for research projects, including tasks related to correlation, regression, logistic regression, and ANOVA Targeted group: Data Analysts/Data scientist: Looking to expand their skills by learning R for data analysis. Students: Interested in starting a career in data analysis or data science and want to learn the basics using R. Researchers and Scholars: Who need to analyze data for their thesis, research projects, or academic papers. Educators and Teachers: Who want to incorporate data analysis and visualization into their curriculum or learn R as a tool for teaching data analysis/science. Business Professionals: Who need to analyze data for their work and want to learn a powerful and versatile programming language for data analysis. Course duration : 30 hours\nPrior knowledge of python: Not required\nTeaching method: Theoretical and hands-on sessions\n","date":1329177600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1721865600,"objectID":"78a98f178f8d9752c15f062c2887af92","permalink":"https://pudasainimohan.com.np/courses/r_analysis/","publishdate":"2012-02-14T00:00:00Z","relpermalink":"/courses/r_analysis/","section":"courses","summary":"This 25 hours immersive workshop equips  with the essential tools to tackle real-world challenges.  With no prior Python experience needed, you'll build a strong foundation in Python programming, explore powerful data manipulation techniques with Pandas, and craft impactful data visualizations using Matplotlib. We'll then introduce you to the exciting world of Machine Learning and Large Language Models (LLMs), sparking your curiosity in these cutting-edge fields.","tags":null,"title":"Mastering  Research  Data  Analysis with  R","type":"docs"},{"authors":null,"categories":null,"content":" To participate in the workshop, please click here.\nIntroduction to Python: Overview of Python and its benefits for data science Setting up your development environment: Installing Python and Jupyter Notebook Introduction to the Jupyter Notebook interface Python Fundamentals: Basic data types: Numbers, Strings, Booleans etc. Descriptive statistics with Python Understanding objects, data types, and methods in Python: Conversion between data types Reading and writing files (local and from the internet) Introduction to Data Manipulation with Pandas: Creating and exploring DataFrames Summarizing data with descriptive statistics Data cleaning, filtering, and transformation techniques Introduction to mutation vs. non-mutation in data manipulation Data Visualization with Python: Introduction to Matplotlib Creating common visualizations: Line charts, Bar charts, Scatter plots, Histograms Customization of plots for clarity and aesthetics Effective storytelling with data visualizations Control Flow with Loops and Functions: Logic control with if-else statements Iterating through data using for and while loops Defining and using functions to improve code reusability Introduction to Machine Learning with Python: Understanding the core concepts of machine learning Exploring different machine learning tasks (classification, regression, etc.) Introduction to popular machine learning libraries in Python (Scikit-learn) Building a simple machine learning model (hands-on example) Introduction to Large Language Models (LLMs) Like: ChatGPT,Gemini: What are LLMs and how do they work? Understanding the potential of LLMs for various applications Details of the dates and times for the programs:\rClick here\n","date":1709938800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1709938800,"objectID":"0788e55ca5dac913dde294e9cb56af19","permalink":"https://pudasainimohan.com.np/courses/python_ml/syllabus/","publishdate":"2024-03-09T00:00:00+01:00","relpermalink":"/courses/python_ml/syllabus/","section":"courses","summary":"To participate in the workshop, please click here.\nIntroduction to Python: Overview of Python and its benefits for data science Setting up your development environment: Installing Python and Jupyter Notebook Introduction to the Jupyter Notebook interface Python Fundamentals: Basic data types: Numbers, Strings, Booleans etc.","tags":null,"title":"Syllabus","type":"docs"},{"authors":null,"categories":null,"content":" To participate in the workshop, please click here.\nBasics R: Introduction of R Interacting with R and R studio R studio Handling Data types Data structure Packages in R and its installation Set working directory Import various types of files in R (such as Excel, STATA, and SPSS) Data cleaning functions -Select, rename, mutate using dplyr, filter, arrange, summaries, group_by, slice,s etc Exporting the data to excel Create a contingency table. The basic concept to create the graph using ggolot2 Argument and properties of ggplot2 Scatter plot Add some elements to the theme and apply the pre-built theme. Boxplot Bar chart Histogram Line chart Basics Statistics Concept: Concept of statistical tests and their terminology (Hypothesis, model fit, Population and Sample, Standard Error, Confidence interval, Effect Size, type of Error) Check Assumptions Exploring the assumption of the parametric test Normality test- Histogram, Density Curve, Q-Q Plot, Skewness, Kurtosis Shapiro Wilk Test, Kolmogorov Smirnov test Homogeneity of variance test- Leven‚Äôs Test Dealing with Outliers Correlation: Carl Pearson‚Äôs Correlation Coefficient and correlation test. Concept of Coefficient of Determination Spearman and Kendall tau Correlation Test Partial Correlation test Point Biserial and Biserial Correlation test Linear Regression: Basics Concept of Regression -Intercept, Coefficient, SST, SSM, SSR, etc Simple regression model Fit and interpret the outcome. Multiple Linear Regression Test the accuracy of the Model Detect the outliers of sample data: Residuals, and standardized residuals. Detect the influential points in sample data Run the multiple regression and interpret the result Durbin Watson test- for Independence of error check VIF- Variance influence factor- For multicollinearity Breusch Pagan test- Homogeneity Graphical Method for assumptions check. Regression with dummy Coding. Logistic Regression: Logistics regression: Concepts Terminology: Log Likelihood, Wald Test, odds, odd ratio Probable Error in Logistics regression: Incomplete Information, Complete separation, overdispersion Binary logistics regression Model fitting Test the accuracy: chi-square test, Pseudo R square Interoperate the model coefficient and odd ratio Case wise diagnostic Testing for multicollinearity Comparing Means(T-test): T-test and its assumptions Independent and dependent t-test Welch t-test Checking the assumption of the t-test Interpretation and application of t-test Analysis of Variance (ANOVA): Introduction to ANOVA Theory of ANOVA Checking the Assumptions of ANOVA Interpretation of ANOVA result Dummy Coding and Contrast setting Post hoc test Effect Size Analysis of Covariance (ANCOVA): Introduction to ANCOVA Procedure of ANCOVA Concept of Sum of squares Interaction effects Interpretation of ANCOVA Contrast in ANCOVA Two-way analysis of Variance: Introduction to two-way ANOVA The procedure of two-way ANOVA Interpretation of two-way ANOVA Details of the dates and times for the programs:\rClick here\n","date":1709938800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1709938800,"objectID":"ab731543920ae27b2fe300b45c45b5bc","permalink":"https://pudasainimohan.com.np/courses/r_analysis/syllabus/","publishdate":"2024-03-09T00:00:00+01:00","relpermalink":"/courses/r_analysis/syllabus/","section":"courses","summary":"To participate in the workshop, please click here.\nBasics R: Introduction of R Interacting with R and R studio R studio Handling Data types Data structure Packages in R and its installation Set working directory Import various types of files in R (such as Excel, STATA, and SPSS) Data cleaning functions -Select, rename, mutate using dplyr, filter, arrange, summaries, group_by, slice,s etc Exporting the data to excel Create a contingency table.","tags":null,"title":"Syllabus","type":"docs"},{"authors":null,"categories":null,"content":" Introduction to Python Overview of Python and its IDEs Installing Python and Jupyter Notebook Introduction to Jupyter Notebook environment Basic Data Types in Python Lists, Dictionaries, Tuples, and Sets Using Python as a calculator Basic arithmetic and logical operations Descriptive statistics Types, Objects, and Methods in Python Understanding objects and data types Converting data types Method syntax Working with Strings in Python Built-in string operations, functions, and methods Reading and writing text files (local and internet) Indexing and slicing strings Numpy Arrays Creating arrays with Numpy Manipulating arrays: indexing, slicing, transforming Understanding the difference between Numpy arrays and Python lists Plotting with Matplotlib Line charts Histograms Bar charts Introduction to the Seaborn library for visualization Data Management with Pandas Summarizing data Cross tabluation Filtering and transforming data mutating Loops and Functions in Python If-Else statements For and While loops Defining functions in Python Details of the dates and times for the programs:\rClick here\n","date":1676329200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1676329200,"objectID":"658c2fff10d9a9fde30db3c5c5aa5cfd","permalink":"https://pudasainimohan.com.np/courses/ds_basics/syllabus_python_dataanalysis/","publishdate":"2023-02-14T00:00:00+01:00","relpermalink":"/courses/ds_basics/syllabus_python_dataanalysis/","section":"courses","summary":"Introduction to Python Overview of Python and its IDEs Installing Python and Jupyter Notebook Introduction to Jupyter Notebook environment Basic Data Types in Python Lists, Dictionaries, Tuples, and Sets Using Python as a calculator Basic arithmetic and logical operations Descriptive statistics Types, Objects, and Methods in Python Understanding objects and data types Converting data types Method syntax Working with Strings in Python Built-in string operations, functions, and methods Reading and writing text files (local and internet) Indexing and slicing strings Numpy Arrays Creating arrays with Numpy Manipulating arrays: indexing, slicing, transforming Understanding the difference between Numpy arrays and Python lists Plotting with Matplotlib Line charts Histograms Bar charts Introduction to the Seaborn library for visualization Data Management with Pandas Summarizing data Cross tabluation Filtering and transforming data mutating Loops and Functions in Python If-Else statements For and While loops Defining functions in Python Details of the dates and times for the programs:\rClick here","tags":null,"title":"Syllabus","type":"docs"},{"authors":null,"categories":null,"content":" Please fill out the following information to register for the course:\nName* Email* Organization* Designation* Mobile Number* Do you have Prior knowledge on R?* Select an option Yes No Submit\nPlease wait a moment after clicking the submit button.\n","date":1721862000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721862000,"objectID":"368d0ed74316c38a7dea18d238beaf6d","permalink":"https://pudasainimohan.com.np/courses/r_analysis/registration-form/","publishdate":"2024-07-25T00:00:00+01:00","relpermalink":"/courses/r_analysis/registration-form/","section":"courses","summary":"Please fill out the following information to register for the course:\nName* Email* Organization* Designation* Mobile Number* Do you have Prior knowledge on R?* Select an option Yes No Submit","tags":null,"title":"Registration form: R workshop","type":"docs"},{"authors":null,"categories":null,"content":"It was a great pleasure to participate as a panelist in the recent global webinar on AI Agents and Bots in Cross-Border Trade, jointly organized by the United Nations ESCAP, the Asian Development Bank (ADB), the International Chamber of Commerce (ICC), and the World Customs Organization (WCO). The session brought together 264 participants from 71 countries, including representatives from Customs administrations, international organizations, academia, and the private sector. The discussions focused on the potential of artificial intelligence agents and bots to streamline and improve the efficiency of cross-border trade processes.\nDuring my presentation, I shared practical use cases demonstrating how AI agents can support various aspects of Customs operations, particularly in the areas of risk management, classifcation, and invoice validation. I also presented findings from my recent research on using AI to identify and expedite environmental goods at Customs, a growing area of importance in the context of green trade facilitation and sustainability.\nThe event offered a valuable platform to learn from other pioneering initiatives. The WCO provided insightful updates on the global adoption of AI in Customs, highlighting both challenges and successful applications. The session also featured a compelling presentation from Germany Customs, showcasing their implementation of AI-powered chatbot and voice assistance.\nAnother highlight was the introduction of ESCAP‚Äôs TINA Platform, an AI-driven trade intelligence solution designed to negotiation of trade agreements by providing insights into current tariffs, non-tariff measures (NTMs), agreements and bilateral trade flows. The private sector also made important contributions, with innovative demonstrations from Complidata and Cuber AI Technologies, emphasizing how scalable AI technologies are being integrated into real-world Customs and trade solutions.\nEvet details: https://www.unescap.org/events/2025/webinar-ai-agents-and-bots-facilitating-paperless-trade-transactions\nThis event was a clear testament to the growing momentum behind AI-powered transformation in trade facilitation. I look forward to continued collaboration and knowledge sharing.\n","date":1750950000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1750950000,"objectID":"5edd58d621195d06374d921a451be1f3","permalink":"https://pudasainimohan.com.np/talk/2025_escap_webinar/","publishdate":"2025-06-29T00:00:00Z","relpermalink":"/talk/2025_escap_webinar/","section":"talk","summary":" had the privilege to speak about the implications of AI in Customs and present real use cases during the webinar on 'AI Agents and Bots for Facilitating Paperless Trade Transactions' organized by UN ESCAP.","tags":[],"title":"Discussed the Role of AI Agents in Customs at UN ESCAP Webinar","type":"talk"},{"authors":null,"categories":null,"content":"I had the privilege of co-facilitating the WCO Risk Management and Data Analytics National Workshop for the State Customs Committee of the Republic of Uzbekistan. The event was held from 16 to 19 June 2025 and conducted under the framework of the SECO-WCO Global Trade Facilitation Programme (GTFP) Phase II.\nThis workshop was co-facilitated with WCO accredited Technical and Operational Advisor Pipat Sirijumrasskul from Thailand Customs.\nDuring the workshop, we focused on:\nAI use cases in the area of Risk Management Hands-on sessions on the WCO DATE model for fraud detection Review of the current risk management status Assistance in drafting a Risk Management Enhancement Plan for Uzbekistan Customs The mission was generously supported by the State Secretariat for Economic Affairs of Switzerland (SECO).\nNews releted to this worksop on WCO website: https://www.wcoomd.org/en/media/newsroom/2025/july/seco-wco-gtfp-phase-ii-holds-workshop-on-risk-management-and-data-analytics-in-uzbekistan.aspx\nNews related to this event is available on the Kazakhstan Customs website: https://customs.uz/uz/news/zhahon-bozhhona-tashkiloti-ekspertlari-ishtirokida-havflarni-boshqarish-va-bozhhonada-malumotlarni-tahlil-qilish-mavzusida-seminar-bolib-otdi\nUzbekistan Customs\u0026rsquo; information on their Instagram regarding the workshop\n","date":1750068000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1750068000,"objectID":"6ab1e122ab511ec12760d21bfa865f1c","permalink":"https://pudasainimohan.com.np/talk/2025_uzbekistan_mission/","publishdate":"2025-06-29T00:00:00Z","relpermalink":"/talk/2025_uzbekistan_mission/","section":"talk","summary":"As a WCO expert, I had the privilege to conduct the WCO Risk Management and Data Analytics Workshop from 16 to 19 June 2025, held in Tashkent, Uzbekistan. During the workshop, we discussed various aspects of Risk Management and Data Analytics.","tags":[],"title":"Delivered WCO Risk Management and Data Analytics Workshop in Uzbekistan","type":"talk"},{"authors":null,"categories":null,"content":"Paperless Trade Week 2025 was organized in Bangkok from 09‚Äì13 June 2025, jointly hosted by United Nations ESCAP, Asian Development Bank (ADB), and over 10 international organizations. This important event brought together trade experts, policymakers, and industry leaders to discuss digital trade facilitation in the evolving global context.\nI was honored to be invited as a panelist in one of the key side events titled ‚ÄúGreen Trade Facilitation and Digital Transformation‚Äù.\nùêäùêûùê≤ ùêèùê®ùê¢ùêßùê≠ùê¨ ùêüùê´ùê®ùê¶ ùêåùê≤ ùêèùêöùêßùêûùê• ùêèùê´ùêûùê¨ùêûùêßùê≠ùêöùê≠ùê¢ùê®ùêß: In my panel discussion, I focused on how Artificial Intelligence (AI) can support green trade facilitation. Key points from my presentation included:\nStrong policy momentum and various global initiatives are driving the shift toward green trade. Expedited clearance of environmental goods is a priority for many Customs administrations. The HS code system presents a challenge, as many green and non-green products share the same codes. Customs must maintain a balance between trade facilitation and compliance. AI presents promising tools for identifying environmental goods and detecting potential fraud. My research developed AI models capable of classifying goods based on their commercial descriptions and flagging suspicious patterns. In the absence of a universal definition, I used lists from three international agreements to train the models. The models demonstrated strong performance and offer practical relevance for Customs risk management. It was a great opportunity to connect with global leaders, international agencies, trade experts, and private sector participants, and to exchange knowledge and experience. I sincerely thank the organizers for the kind invitation and for hosting such fruitful and forward-thinking discussions.\nMore details about the program UNESCAP website.\n","date":1749463200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1749463200,"objectID":"3687f94f647deb718320fea475ffe2eb","permalink":"https://pudasainimohan.com.np/talk/2025_paperless_tradeweek/","publishdate":"2025-06-13T00:00:00Z","relpermalink":"/talk/2025_paperless_tradeweek/","section":"talk","summary":"I had the privilege to participate as a panelist in the UNESCAP flagship event, Paperless Trade Week 2025, where I shared insights on how Artificial Intelligence (AI) can support green trade facilitation by improving the identification of environmental goods and balancing trade compliance and facilitation goals.","tags":[],"title":"Participated as Panelist at Paperless Trade Week 2025","type":"talk"},{"authors":null,"categories":null,"content":"I had the privilege of co-facilitating the WCO Risk Management National Workshop for the State Revenue Committee (SRC) of Kazakhstan, held at the Radisson Hotel in Astana from 12 to 14 May 2025. The workshop had 30 participants from different Customs offices of Kazakhstan Customs. The workshop focused on various aspects of risk management such as trader categorization, insider threats, international postal stream, and interactive risk profiling dashboards. One of the most important parts was the incorporation of Artificial Intelligence (AI) approaches into Customs risk management and use cases of AI in various Customs administrations.\nThis workshop was conducted with two other WCO Risk Management Experts, Anders Alpsten from Sweden Customs and Sreya Hong of Cambodia Customs. It was a unique opportunity to collaborate, share experiences, and support Kazakhstan Customs in its journey toward more modern and resilient risk management systems. The mission was generously supported by the Customs Cooperation Fund (CCF) China.\nNews related to this event is available on the Kazakhstan Customs website: https://kgd.gov.kz/en/content/regional-training-center-world-customs-organization-astana-1-3\n","date":1747044000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1747044000,"objectID":"291698a167b364303cd81c0b8b481bac","permalink":"https://pudasainimohan.com.np/talk/2025_kazakhstan_mission/","publishdate":"2025-05-25T00:00:00Z","relpermalink":"/talk/2025_kazakhstan_mission/","section":"talk","summary":"I had the privilege of co-facilitating the WCO Risk Management National Workshop for the State Revenue Committee (SRC) of Kazakhstan, held at the Radisson Hotel in Astana from 12 to 14 May 2025. The workshop had 30 participants from different Customs offices of Kazakhstan Customs.","tags":[],"title":"Delivered WCO Risk Management Workshop in Kazakhstan","type":"talk"},{"authors":null,"categories":null,"content":"At the 11th National Summit of Health and Population Scientists in Nepal, held from 10‚Äì12 April 2025 at Soaltee Hotel, Kathmandu, I presented a co-authored research paper on 10 April.The event was organized by Nepal Health Research Council(NHRC). Our study focused on using mobile-based Ecological Momentary Assessment (EMA) and machine learning to predict suicidal ideation among gay, bisexual, and other men who have sex with men (GBMSM).\nWe collected real-time data through smartphones to monitor mood, context, and behavior. These data were then analyzed using machine learning models to identify patterns linked to suicidal thoughts. The goal is to enable early detection and personalized mental health support for a group that faces significant stigma and mental health challenges.\nThis presentation was delivered in Nepali and contributes to ongoing efforts to apply technology for inclusive, data-driven public health solutions in Nepal.\nPresentation video (57:50 to 1:08:11)\n","date":1744279200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1744279200,"objectID":"4899fbf83c5fb0415a3a1a62b202875d","permalink":"https://pudasainimohan.com.np/talk/2025_nhrc_summit/","publishdate":"2026-05-01T00:00:00Z","relpermalink":"/talk/2025_nhrc_summit/","section":"talk","summary":"On 10 April 2025, I co-authored and presented our research at the 11th National Summit of Health and Population Scientists in Nepal, held at Soaltee Hotel, Kathmandu. Our study combines mobile-based Ecological Momentary Assessment (EMA) and machine learning to predict suicidal ideation among gay, bisexual, and other men who have sex with men (GBMSM).","tags":[],"title":"Paper on Suicidal Ideation Prediction Presented at NHRC Summit 2025","type":"talk"},{"authors":null,"categories":null,"content":"On March 4, 2025, I had the privilege of leading a session on \u0026lsquo;Data Journalism\u0026rsquo; training organized by CMR Nepal - Journalism Academy. The session brought together more than 15 journalists from various media.where I discussed trade and financial data, exploring their sources, significance, and real-world applications in data-driven reporting in the context of Nepal.\nWhat made this experience particularly rewarding was the active engagement and insightful contributions from the participants. Their enthusiasm reaffirmed the pivotal role of data-driven journalism in today‚Äôs media landscape.\nI\u0026rsquo;m grateful for the opportunity to contribute to this important training initiative and appreciate CMR Nepal - Journalism Academy for making it possible!\n","date":1741078800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1741078800,"objectID":"1a711f0a507c48ad56bc071633dbb3c6","permalink":"https://pudasainimohan.com.np/talk/2025_datajournalism/","publishdate":"2025-03-05T00:00:00Z","relpermalink":"/talk/2025_datajournalism/","section":"talk","summary":"I had the privilege of leading a session on 'Data Journalism' training organized by CMR Nepal - Journalism Academy. where I discussed trade and financial data, exploring their sources, significance, and real-world applications in data-driven reporting in the context of Nepal.","tags":[],"title":"Exploring Trade and Financial Data in Journalism at CMR Nepal - Journalism Academy","type":"talk"},{"authors":["Mohan Kumar Pudasaini"],"categories":["Customs"],"content":"International Customs Day is celebrated each year on the 26th of January and regards the customs administrations\u0026rsquo; critical functions such as societal and economic security, trade facilitation, and economic prosperity. Every year, the World Customs Organization (WCO) puts out its International Customs Day theme to underscore the most recent challenges and opportunities regarding customs. The WCO theme for 2025 is \u0026ldquo;Customs Delivering on its Commitment to Efficiency, Security, and Prosperity\u0026rdquo;. This theme indicates the commitment to the development and modernization of customs processes around the globe.\nNepal Customs has made notable progress in using digital tools and automation, with ASYCUDA World as the main system for declarations in major customs offices. Many modules support data-based operations, including risk-based targeting, creating risk profiles, forecasting trade and revenue, valuation, and providing trade statistics to the public. The launch of a National Single Window system has made trade easier by allowing regulatory bodies to issue licenses, permits, certificates, and other documents online. A notable development in the utilization of AI in the Nepal Customs is the application of AI driven Harmonic code search.\nAchieving effective risk management and data analytics has been one of the cornerstones of improving efficiency, transparency, and compliance in customs. The building blocks of big data and artificial intelligence and machine learning enables decision-making, risk assessment, and resource allocation within the Customs to be more effective. Data driven techniques help mitigate the unidentified and managed trends, patterns, and trade compliance risks. They also facilitate the targeting and profiling of different threats on the basis of risks. These improvements augment the effectiveness of transparency and accountability in customs.\nEven after these changes took place, Nepal Customs have some challenges during the process of a fully data rich environment. The issues such as legal framework on incorporation of AI in customization of frameworks, data strategy, data quality, infrastructure and capacity building need to be resolved for the effective use of emerging technologies. The customs personnel need orientation and capacity building to achieve maximum benefit out of data analytics. It is imperative to foster an innovation culture within the organization because when the innovation change resistance is removed progress is continuously made.\nEnhancing security measures is another priority for Nepal Customs in the fight against illicit trade and smuggling. With respect to these activities the non intrusive inspection, enhanced border control and especially working together with international partners will earn paramount importance. Furthermore, active stance towards crisis set up, especially environmental and cyber crisis, will build more strength and maintain the ongoing stability.\nAs global trade continues to transform, Nepal Customs is at a crucial point both for prospects and for problems. Achieving modernization is not only about utilizing new techniques, but also about enabling an entire system that can integrate innovation and economic growth while ensuring safety. Nepal‚Äôs customs operations can be transformed into a powerful engine for national economic development by purposefully putting resources into the digital infrastructure, enhancing the ability of people, and cultivating a culture of constant change. The way forward is an integrated, comprehensive, and bold approach which puts emphasis on the inclusion of technology, and at the same time preserving the systems that rely on human skill for positive impact.\n","date":1737849600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1737849600,"objectID":"a567e6f1134eec2b86724721bce14ed1","permalink":"https://pudasainimohan.com.np/post/icd_2025/","publishdate":"2025-01-26T00:00:00Z","relpermalink":"/post/icd_2025/","section":"post","summary":"International Customs Day is celebrated each year on the 26th of January and regards the customs administrations' critical functions such as societal and economic security, trade facilitation, and economic prosperity. Every year, the World Customs Organization (WCO) puts out its International Customs Day theme to underscore the most recent challenges and opportunities regarding customs. The WCO theme for 2025 is \"Customs Delivering on its Commitment to Efficiency, Security, and Prosperity\". This theme indicates the commitment to the development and modernization of customs processes around the globe.","tags":["Customs","Risk Management","Trade Facilitation","International Customs Day","ICD","Nepal Customs"],"title":"Celebrating International Customs Day with Nepal's Path to Modernization","type":"post"},{"authors":null,"categories":null,"content":"I am thrilled to share my experience attending the World Customs Organization (WCO) First Pre-Accreditation Workshop on the area of Technical and Operational Advisers in Data Analytics, held in South Korea from January 13 to January 17, 2025. This workshop was a significant milestone in my professional journey, as it focused on evaluating various aspects of technical expertise and facilitation skills.\nDuring the workshop, I had the opportunity to engage in comprehensive assessments and activities designed to measure and enhance our capabilities as prospective WCO experts. I am honored to have been selected to proceed to the next stage of the accreditation process: a field mission. Successfully completing this process will enable participants to join the esteemed pool of WCO experts, collaborating with the Organization to benefit its Member states.\nA particularly memorable moment during this workshop was the celebration of my birthday. The thoughtful gesture by the team made the occasion truly special, and I am deeply grateful to them for their kindness and support.\nThis experience has not only paved the way for my continued professional growth but also strengthened my resolve to contribute meaningfully to the customs community. I look forward to the upcoming stages of the accreditation process and the opportunity to work alongside distinguished experts from around the world. News in WCO website: The first-ever WCO Pre-Accreditation Workshop focused on Data Analytics holds in Korea\n","date":1736758800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1736758800,"objectID":"c6176846e4ab3bfe641b80d0e5fec965","permalink":"https://pudasainimohan.com.np/talk/2025_da_accreditation/","publishdate":"2025-01-18T00:00:00Z","relpermalink":"/talk/2025_da_accreditation/","section":"talk","summary":"I am thrilled to share my experience attending the World Customs Organization (WCO) First Pre-Accreditation Workshop, held in South Korea from January 13 to January 17, 2025. This workshop was a significant milestone in my professional journey, as it focused on evaluating various aspects of technical expertise and facilitation skills.","tags":[],"title":"Advancing Expertise: WCO Pre-Accreditation in Data Analytics","type":"talk"},{"authors":["Mohan Kumar Pudasaini"],"categories":["Data Analytics","Customs"],"content":"As 2024 draws to a close, I‚Äôm excited to reflect on a year filled with growth, contributions, and meaningful engagements. Alongside my regular responsibilities at Nepal Customs, I had the privilege of participating in diverse initiatives that expanded my skills, built valuable networks, and contributed to global efforts in customs and trade facilitation.\nKey Achievements in 2024 1. Building Networks and Sharing Knowledge Organizing Workshops: I personally organized two comprehensive 30-hour online workshops to foster a data-driven culture and strengthen networks in Nepal where I trained more than 200 participants from various sectors: Data Analysis and Machine Learning with Python Research Data Analysis with R 2. Gaining Skills and Knowledge BACUDA Scholarship Program: At the start of the year, I participated in the BACUDA Scholarship Program in Korea, where I gained invaluable insights into data analytics, artificial intelligence (AI), and their applications in customs and trade facilitation. 3. Contributing to WCO Initiatives WCO-UNESCAP 6th UNNExT Masterclass: As a trainer, I shared practical tools and techniques on data quality, risk assessment, and advanced analytics with customs officials from over 20 countries. Learn more about the Masterclass WCO Working Group on Data and Statistics: I contributed to discussions on AI models and their practical applications in customs operations. Details of the programme 4. Advancing AI and Data Analytics in Nepal Supporting Nepal‚Äôs Supreme Audit Institution: Continuing my role as an AI and Data Analytics Specialist for the Office of the Auditor General, I have been working on various research and data analytics to enhance auditing processes. Leading Sessions: I led various sessions on AI, data analytics, and customs-related topics, contributing to professional development and capacity building: Workshop on Data Analysis Using R at Quest International College AI Innovations at the Office of the Auditor General Exploring AI in Customs at the Public Finance Management Training Center Advancing Customs Practices at the Public Finance Management Training Center Lessons Learned This year has underscored the importance of leveraging data-driven approaches to drive innovation and efficiency. Engaging with global and local professionals has highlighted the value of collaboration, continuous learning, and adaptability in addressing complex challenges.\nLooking Ahead to 2025 As I step into 2025, my focus remains on making meaningful contributions to the customs community and advancing the ethical use of AI:\nContributions to Customs Community: Applying data analytics, AI and risk management techniques to enhance operational efficiency and facilitate trade. Supporting WCO Initiatives: Actively engaging in WCO‚Äôs global efforts to modernize customs practices. Advocating for Ethical AI: Promoting transparency, accountability, and inclusivity in AI implementations. Expanding Training Programs: Developing workshops and courses to empower professionals with advanced skills in data analytics and AI. Commitment to Continuous Learning: Embracing lifelong learning and expanding knowledge horizons to stay at the forefront of innovations in customs and trade. ","date":1735603200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1735603200,"objectID":"e4dea95b7401c89e5ce69f574ba50785","permalink":"https://pudasainimohan.com.np/post/summary_2024/","publishdate":"2024-12-31T00:00:00Z","relpermalink":"/post/summary_2024/","section":"post","summary":"2024 has been a transformative year, marked by significant achievements alongside my core responsibilities at Nepal Customs. I contributed to global customs initiatives through the WCO and led workshops in Nepal to build a data-driven culture. Highlights include participating in the BACUDA Scholarship Program, serving as an AI Specialist at the Auditor General's Office, and training professionals on AI and data analytics. In 2025, I aim to further enhance AI and risk management in customs, support WCO initiatives, and advocate for ethical AI practices.","tags":["Customs","AI","DataAnalytics","Intelligence","Risk Management","Trade Facilitation"],"title":"Reflecting on 2024 and Looking Ahead to 2025","type":"post"},{"authors":null,"categories":null,"content":"On December 26, 2024, I had the privilege of leading a session on \u0026ldquo;Data Analytics and AI in Customs\u0026rdquo; at the Public Finance Management Training Center, Nepal. This event brought together more than 35 officials from various Customs and Tax offices, creating an enriching environment for knowledge-sharing and collaboration.\nDuring the session, we delved into global trends, key World Customs Organization (WCO) initiatives, and the practical applications of data analytics and artificial intelligence within the customs. Key focus areas included risk management, fraud detection, and trade facilitation. The session also highlighted how Nepal Customs is integrating these advanced technologies into its operations to improve efficiency and compliance.\nWhat made this experience particularly fulfilling was the active engagement and insightful contributions from the participants. This exchange of ideas reaffirmed the pivotal role of data-driven approaches in shaping the future of customs and tax administration.\nI\u0026rsquo;m grateful for the opportunity to contribute to this important training initiative.\n","date":1735207200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1735207200,"objectID":"be16bf69281788e2601e8e8d9b219080","permalink":"https://pudasainimohan.com.np/talk/2024_pfmtc1/","publishdate":"2024-12-26T10:00:00Z","relpermalink":"/talk/2024_pfmtc1/","section":"talk","summary":" I had  the honor of leading a session on Data Analytics and AI in Customs at the Public Finance Management Training Center, Nepal. This session engaged over 35 officials from Customs and Tax offices, offering insights into global trends, WCO initiatives, and Nepal's practical applications of advanced technologies. Discussions focused on risk management, fraud detection, and enhancing trade facilitation, underscoring the transformative power of data-driven approaches in modern customs and tax operations.","tags":[],"title":"Advancing Customs and Tax Practices with Data Analytics and AI","type":"talk"},{"authors":["Mohan Kumar Pudasaini"],"categories":["Data Analytics","Customs"],"content":"Global trade has undergone a remarkable transformation in recent years. Merchandise trade volumes have steadily increased since 2015 and have been forecasted to grow in future years, showcasing the resilience of global trade systems. The sharp V-shaped recovery following the disruptions caused by COIVD 19 emphasizes how adaptable these systems have become, largely due to advancements in technology and data-driven decision-making. Figure Source: World Trade Organization (2024), Global Trade Outlook and Statistics\nModern customs authorities must adapt to the complexities of increased trade flows while ensuring efficiency, security, and compliance.\nThe Data Challenge in Modern Customs With global e-commerce surpassing $1 trillion annually and air passenger traffic recovering rapidly, customs administrations are inundated with data from multiple sources. Effectively leveraging these data streams is critical for meeting operational demands.\nKey Customs Data Sources Customs authorities process massive volumes of information generated across multiple domains, including:\nImport and Export Declarations: Containing detailed information about goods, parties involved, and declared values. Cargo Manifests: Providing logistics and carrier details critical for tracking shipments. Advanced Passenger Information (API) \u0026amp; Passenger Name Records (PNR): Enabling effective risk assessment of travelers. LPCO: License, Permit, Certificate, and Other Approvals,Supporting trade facilitation and regulatory compliance. Historical Compliance Records and Enforcement Data: Enhancing risk profiling and fraud detection. Commercial Invoices \u0026amp; Payment Information: Offering transparency in financial transactions. Supply Chain Documentation \u0026amp; Tracking Data: Helping customs track goods across borders. Customs operations must harness these diverse data streams to uncover actionable insights while managing the growing complexity of international trade.\nBuilding a Smarter Data Ecosystem As data complexity increases, the ability to store, process, and analyze data becomes a key challenge. Advanced technologies like data lakes have emerged as a flexible solution, accommodating structured, semi-structured, and unstructured data. Data governance, security, and scalability are vital for building a reliable ecosystem to support customs operations.\nLeveraging Advanced Analytics and AI Advanced analytics and artificial intelligence (AI) are transforming customs operations, offering:\nReal-Time Risk Assessment: AI-powered risk scoring systems for fraud detection and predictive analytics. Automated Document Processing: Tools like OCR and natural language processing (NLP) for faster, more accurate trade document reviews. Valuation and Classification: Automated HS code classification and verification systems using machine learning. Smart Audit Systems: Risk-based audit selection and compliance monitoring using data analytics. Leading Innovations by the World Customs Organization (WCO) The WCO is at the forefront of technological adoption in customs. Key initiatives include:\nAI-Powered HS Classification Systems: Automating the classification of goods for customs duties. Fraud Detection Models (DATE): Identifying patterns and anomalies in trade data. Advanced X-Ray Image Analysis: Using AI for accurate detection of contraband in cargo inspections. Smart Customs Projects: Addressing member countries\u0026rsquo; technological challenges through innovative solutions. The Path Forward The intelligent integration of these technologies will shape the future of customs management. For successful implementation, customs administrations need to focus on:\nRobust Data Strategies: Including standardization, secure sharing protocols, and governance frameworks. Technology Integration: Scalable systems that are secure and user-friendly. Workforce Development: Upskilling staff in analytical techniques, AI tools, and data management. Conclusion The transformation of customs through data analytics and AI is not just a response to the increasing complexity of trade but a proactive step toward building smarter, more adaptive systems. These advancements will enable customs administrations to streamline processes, enhance security, and maintain compliance. As global commerce continues to evolve, customs administrations must remain at the forefront of innovation. The ability to harness emerging technologies and leverage diverse data sources will be the cornerstone of efficient and effective border management in the years to come.\n","date":1732320000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1732320000,"objectID":"5544b56df8c4bc39a829a474d98b37e4","permalink":"https://pudasainimohan.com.np/post/customs_20241123/","publishdate":"2024-11-23T00:00:00Z","relpermalink":"/post/customs_20241123/","section":"post","summary":"Global trade has shown remarkable resilience, particularly in its V-shaped recovery following the disruptions caused by COVID-19, driven by technological advancements and data-driven decision-making. Modern customs authorities face challenges managing complex data streams from various sources, such as import/export declarations and passenger information. Leveraging advanced analytics, AI, and technologies like data lakes enhances efficiency, security, and compliance. Innovations like AI-powered HS classification and fraud detection models are transforming customs operations. Moving forward, robust data strategies, technology integration, and workforce development will be essential for customs administrations to navigate the complexities of post-pandemic international trade.","tags":["Customs","AI","DataAnalytics","Intelligence","Risk Management","Trade Facilitation"],"title":"Building Smarter Customs: A Data Analytics and AI Perspective","type":"post"},{"authors":["Mohan Kumar Pudasaini"],"categories":["OSINT"],"content":"In today\u0026rsquo;s digital age, many customs administrations are turning to Open Source Intelligence (OSINT) to enhance traditional intelligence-gathering methods. OSINT involves collecting and analyzing publicly available information from sources such as social media, websites, public records, and news outlets to generate actionable intelligence. Although the concept of OSINT is not new, dating back to World War I when nations recognized the value of intercepting open communications, its application has evolved significantly with advancements in technology. Today, customs authorities leverage OSINT to identify potential threats, monitor trade patterns, and ensure regulatory compliance, making it an essential tool for modern risk management and trade facilitation.\nThe Evolution of OSINT in Customs Historically, customs operations relied heavily on physical inspections and paper-based processes. However, the rise of the internet and global connectivity has transformed these practices. OSINT now enables authorities to monitor online platforms, track digital interactions, and uncover suspicious activities that were once invisible. This shift has been driven by the proliferation of user-generated data on social media, increasing demand for transparency from businesses, and a global surge in internet usage. As the volume of digital data grows, OSINT has become an essential component of modern customs intelligence. For instance, customs officials can now monitor social media to detect patterns of trade fraud or use AI-powered tools to cross-check trade documents with public data to ensure compliance.\nKey OSINT Sources in Customs Administration Digital Footprints Social Media Platforms: Networks like Facebook, Twitter/X, Instagram, and LinkedIn provide insights into trade patterns and suspicious activities.\nCorporate Websites and Press Releases: Help verify company legitimacy and reveal partnerships or financial information.\nOnline Marketplaces: Platforms such as Amazon and Alibaba can be analyzed for price reference, counterfeit goods and illegal trade.\nShipping and Logistics Forums: Expose abnormal trade routes and hidden smuggling networks.\nPublic Records Government Databases: Access to trade registries and tax records aids in compliance verification. Court Records: Reveal previous trade violations and suspicious legal activities. Geotagged Data and Satellite Imagery: Geospatial intelligence (GEOINT) helps map smuggling routes and identify hotspots. Media and Academic Sources News Articles: Provide real-time updates on emerging threats and sanctions. Industry Publications: Offer sector-specific insights and market analyses. Academic Research: Studies reveal trade patterns and supply chain vulnerabilities. Applications of OSINT in Modern Customs Operations Risk Assessment and Profiling: OSINT enables comprehensive risk profiling by analyzing historical trade patterns, monitoring social media for suspicious behavior, and leveraging predictive analytics to anticipate high-risk periods. Fraud Detection: Customs authorities use OSINT to detect trade-based money laundering, price manipulation, and shell companies by cross-referencing documents with online data to identify discrepancies. Smuggling Prevention: Authorities employ advanced OSINT techniques such as: Real-time monitoring of shipping movements. Geospatial mapping of high-risk routes. Integration of satellite imagery with trade data to track illegal goods. Technical Implementation Tools and Technologies Data Scraping: Automates data extraction from online sources. Social Media Monitoring Platforms: Tracks real-time activities across various networks. Geospatial Mapping Systems: Combines satellite data with OSINT for enhanced analysis. Machine Learning Algorithms: Identifies patterns and anomalies for effective risk detection. Integration with Existing Systems Successful OSINT implementation requires seamless integration with risk management platforms, real-time data processing capabilities, and secure information-sharing protocols.\nLegal and Ethical Considerations Customs authorities must comply with data protection regulations like GDPR while respecting privacy laws. Implementing multi-source verification is essential to ensure the accuracy of collected data. Adhering to ethical standards is crucial for the responsible use of public information.\nBest Practices for Effective OSINT Deployment To maximize the benefits of OSINT:\nEstablish Dedicated OSINT Units: Form specialized teams trained in data analysis and intelligence gathering. Regular Source Evaluation: Continuously assess the relevance and credibility of data sources. Cross-referencing Data: Validate OSINT findings with official records to ensure reliability. Challenges in OSINT Implementation Information Overload: Utilize AI-powered filtering tools and automated alerts to focus on relevant data. Data Quality Issues: Implement multi-layer verification processes alongside source credibility scoring. Conclusion OSINT has revolutionized customs operations by providing real-time insights that enhance risk assessment, fraud detection, and smuggling prevention. As technology continues to evolve, the importance of OSINT will only grow. Customs authorities must stay updated with the latest tools while adhering to ethical standards. Proper implementation of OSINT strategies ensures secure global trade while effectively combating illicit activities.\n","date":1729641600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1729641600,"objectID":"c4ec25fd11b58efc5953d7e5760f7328","permalink":"https://pudasainimohan.com.np/post/osint_20241023/","publishdate":"2024-10-23T00:00:00Z","relpermalink":"/post/osint_20241023/","section":"post","summary":"Open Source Intelligence (OSINT) is revolutionizing customs operations by utilizing publicly available data from digital platforms, public records, and media to enhance risk assessment, fraud detection, and smuggling prevention. As technology advances, customs authorities can leverage tools like data scraping, social media monitoring, and geospatial mapping to gain real-time insights and identify suspicious trade activities. However, successful implementation requires adherence to legal and ethical standards, along with continuous evaluation of data sources and integration with existing systems to ensure secure and effective global trade","tags":["Customs","AI","OSINT","Intelligence","Risk Management","Opensource Intelligence","Trade Facilitation"],"title":"OSINT: Revolutionizing Customs Intelligence in the Digital Age","type":"post"},{"authors":null,"categories":null,"content":"I was honored to present at the 5th meeting of the WCO Working Group on Data and Statistics. My presentation, titled \u0026ldquo;Harnessing AI for Customs Efficiency: A BACUDA Experience,\u0026rdquo; focused on showcasing various AI models developed as part of the BACUDA program and their applications within the customs sector.\nDuring the presentation, I highlighted the journey of developing AI tools, including chatbot assistance, HS classification from images, fraud detection, and automatic document processing. I shared insights into how these models are designed to streamline customs operations, enhance risk management, and improve compliance. Additionally, I discussed the deployment of AI-based systems within Nepal Customs, such as the AI-HS classification tool integrated into the Nepal National Single Window Portal, and how these initiatives reflect global trends promoted by the World Customs Organization (WCO).\nThe opportunity to present on this international platform allowed me to share our achievements, learn from fellow experts, and contribute to the broader conversation on leveraging data analytics and AI for customs efficiency. I am grateful to the WCO and the BACUDA program for facilitating this enriching experience.\n","date":1728295200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728295200,"objectID":"83e0325673f919d7703299c47a8ad54e","permalink":"https://pudasainimohan.com.np/talk/2024_wgds/","publishdate":"2024-10-08T00:00:00Z","relpermalink":"/talk/2024_wgds/","section":"talk","summary":"I had the privilege to present at the 5th meeting of the WCO Working Group on Data and Statistics. My presentation, titled Harnessing AI for Customs Efficiency: A BACUDA Experience, focused on showcasing AI models developed within the BACUDA program and their applications in the customs sector. This opportunity allowed me to represent Nepal and discuss innovative customs solutions. Thanks to the WCO and WGDS for this incredible opportunity!.","tags":[],"title":"Presented at the  WCO Working Group on Data and Statistics: Harnessing AI for Customs Efficiency - A BACUDA Experience","type":"talk"},{"authors":null,"categories":null,"content":"On August 9, 2024, I had the opportunity to lead a session on \u0026ldquo;Data Analytics and AI in Customs\u0026rdquo; at the Public Finance Management Training Center in Nepal. This session was part of an in-service training program designed for gazetted third-class officers of the Ministry of Finance and their departments. It focused on providing insights into global trends, WCO initiatives, and practical applications of data analytics and artificial intelligence within the customs sector.\nDuring the session, we explored how data analytics and AI are revolutionizing customs operations worldwide, driving efficiency, and supporting trade facilitation. I shared information on key initiatives by the World Customs Organization (WCO) that aim to integrate these advanced technologies into customs practices. We also discussed various applications, such as risk management and fraud detection, and how these concepts are being implemented within Nepal Customs to enhance operations.\nEngaging with officers from various departments made the session especially rewarding, as it facilitated a meaningful exchange of ideas about the future of customs in a data-driven era. I appreciated the active participation and insightful discussions throughout the training.\n","date":1723197600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1723197600,"objectID":"d5d46d5642b9ec742e4456fa0d7f00b6","permalink":"https://pudasainimohan.com.np/talk/2024_pfmtc/","publishdate":"2024-08-10T00:00:00Z","relpermalink":"/talk/2024_pfmtc/","section":"talk","summary":"I had the privilege of being invited as a resource person for the presentation on Data Analytics and AI in Customs at the Public Finance Management Training Center, Nepal on August 9, 2024. This session was part of a training program aimed at equipping gazetted third-class officers of the Ministry of Finance and their departments with insights into global trends, WCO initiatives, and practical applications of data analytics and artificial intelligence within the customs sector.","tags":[],"title":"Exploring Data Analytics and AI in Customs at Public Finance Management Training Center","type":"talk"},{"authors":null,"categories":null,"content":"‚ÄÉFee Rs 3000 only\nSyllabus‚ÄÉClick here Registration‚ÄÉClick here\nFor additional information 9845074610\nR has become an essential tool for data scientists and analysts, providing a comprehensive suite of capabilities for analyzing and visualizing data. Whether you are new to data analysis or looking to expand your skills with R, this course is tailored for you. Designed for individuals from diverse backgrounds, this course focuses on the basics of R programming, data analysis, and visualization, without requiring prior coding experience.\nTarget Audience:\nData Analysts/Data Scientists: Currently working with different software and looking to expand their skills by learning R for data analysis. Students: Interested in starting a career in data analysis or data science and want to learn the basics using R. Researchers and Scholars: Who need to analyze data for their thesis, research projects, or academic papers. Educators and Teachers: Who want to incorporate data analysis and visualization into their curriculum or use R as a tool for teaching data analysis. Business Professionals: Who need to analyze data for their work and want to learn a powerful and versatile programming language for data analysis. By the end of the course, participants will have a strong foundation in R programming and the ability to perform data analysis, visualization, and statistical modeling tasks. With hands-on experience using RStudio and various R libraries covered in the course, you\u0026rsquo;ll be equipped to tackle real-world data analysis projects. For registration, please Click here .\nDon\u0026rsquo;t miss this opportunity to learn essential skills in the data analysis field. This comprehensive course offers great value for anyone looking to become proficient in R. Enroll now to elevate your data analysis capabilities to the next level!\n","date":1722801600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1722801600,"objectID":"54bae79897562dc20848028bfe348790","permalink":"https://pudasainimohan.com.np/talk/2024_rworkshop/","publishdate":"2024-07-25T00:00:00Z","relpermalink":"/talk/2024_rworkshop/","section":"talk","summary":"This 30-hour immersive course equips participants with the essential tools to tackle real-world challenges. No prior experience with R is required, making it accessible to beginners and those looking to expand their skillset. You will build a strong foundation in R programming, explore powerful data manipulation techniques , and create impactful data visualizations. Additionally, the course will cover advanced statistical methods, including regression analysis and ANOVA, to provide a comprehensive understanding of data analysis techniques.\n**The course will take place via Zoom from 8:00 PM to 9:30 PM, starting on August 4th (20 Shrawan 2081)**","tags":[],"title":"Mastering  Research  Data  Analysis with  R","type":"talk"},{"authors":null,"categories":null,"content":"On July 7, 2024, I had the honor of speaking at an event organized by the Supreme Audit Institution of Nepal, The Office of Auditor General\u0026rsquo;s. This event was attended by government officials from the OAG office who were keen on exploring the potential of AI in auditing processes.\nDuring my session, I provided an overview of artificial intelligence, tracing its timeline and highlighting the latest developments in the field. I also discussed the possible applications of AI in auditing, emphasizing the opportunities it presents for improving efficiency and accuracy in auditing processes. Additionally, I addressed the challenges and considerations that come with integrating AI technologies in such sensitive and crucial areas.\nThe session was well-received, with participants engaging in discussions about the practical implications and future possibilities of leveraging AI in their work. It was a rewarding experience to contribute to the understanding and exploration of AI innovations within such a pivotal institution.\n","date":1720346400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720346400,"objectID":"7a646313803ced8a968f6a2ecbccf3e2","permalink":"https://pudasainimohan.com.np/talk/2024_oag/","publishdate":"2024-07-01T00:00:00Z","relpermalink":"/talk/2024_oag/","section":"talk","summary":"On July 7, 2024, I spoke at an event organized by OAG. The session focused on the potential of AI in auditing, covering its timeline, recent advancements, and practical applications. I also discussed the opportunities and challenges of integrating AI into auditing processes. The event was attended by government officials, and the discussions highlighted the promising role of AI in enhancing auditing efficiency and accuracy.","tags":[],"title":"Talking AI Innovations at the Auditor General's Office","type":"talk"},{"authors":null,"categories":null,"content":"I had the privilege of being invited as a resource person for the \u0026ldquo;Ultimate Workshop on Data Analysis Using R\u0026rdquo; organized by Quest International College, Nepal on June 29, 2024. This workshop focused on equipping participants with the skills needed to perform advanced data analysis using R.\nDuring the workshop, I conducted a session on regression analysis, covering both linear regression and logistic regression. I explained the theoretical interrelationship between these two types of regression, how to apply these concepts using R, and the steps involved in data preparation and exploratory data analysis (EDA). Additionally, I guided participants through model development, model comparison, and the list of assumptions associated with these models. Finally, I demonstrated how to analyze and interpret the results.\nThe workshop was attended by over 30 participants, including lecturers from various colleges. It was an enriching experience, and I was delighted to share my knowledge and expertise with such an enthusiastic group of learners.\n","date":1719655200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1719655200,"objectID":"da3cc7995c32fe62a762c58923cb1733","permalink":"https://pudasainimohan.com.np/talk/2024_quest/","publishdate":"2024-07-01T00:00:00Z","relpermalink":"/talk/2024_quest/","section":"talk","summary":"I had the privilege of being invited as a resource person for the 'Ultimate Workshop on Data Analysis Using R' organized by Quest International College, Nepal on June 29, 2024. This workshop focused on equipping participants with the skills needed to perform advanced data analysis using R.","tags":[],"title":"The Ultimate Workshop on Data Analysis Using R at Quest International College","type":"talk"},{"authors":null,"categories":null,"content":"‚ÄÉFee Rs 3000 only\nSyllabus‚ÄÉClick here\nPython has become a critical tool for data scientists and analysts, offering a vast range of capabilities for analyzing and visualizing data. Whether you are a beginner or looking to enhance your existing skills in python, then this workshop is for you. This workshop is designed for individuals from a variety of backgrounds who have an interest in data analysis and visualization and want to learn the basics of Python programming without prior coding knowledge. The workshop is specifically targeted towards:\nData Analysts/Data scientist: Currently working with different software and looking to expand their skills by learning Python for data analysis. Students: Interested in starting a career in data analysis or data science and want to learn the basics of data analysis using Python. Researchers and Scholars: Who need to analyze data for their thesis, research projects, or academic papers. Educators and Teachers: Who want to incorporate data analysis and visualization into their curriculum or learn Python as a tool for teaching data science. Business Professionals: Who need to analyze data for their work and want to learn a powerful and versatile programming language for data analysis. By the end of the workshop, you will have a solid foundation in Python programming and the ability to use Python for various data analysis , visualization and machine learning tasks. With hands-on experience using Jupyter Notebook and the various libraries used in the workshop, you will be ready to tackle real-world data analysis projects. To view the full syllabus, please Click here .\nDon\u0026rsquo;t miss out on this opportunity to learn the most in-demand skills in the data analysis field. With a low fee, this comprehensive beginner\u0026rsquo;s workshop offers exceptional value for anyone looking to become a proficient Python data analyst. Enroll now to take your skills to the next level!\n","date":1710964800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1710964800,"objectID":"6d57fd783033909270928e49a6e3c4e1","permalink":"https://pudasainimohan.com.np/talk/2024_python/","publishdate":"2024-03-09T00:00:00Z","relpermalink":"/talk/2024_python/","section":"talk","summary":"This 25 hours immersive workshop equips  with the essential tools to tackle real-world challenges.  With no prior Python experience needed, you will build a strong foundation in Python programming, explore powerful data manipulation techniques with Pandas, and craft impactful data visualizations using Matplotlib. We will then introduce you to the exciting world of Machine Learning and Large Language Models (LLMs), sparking your curiosity in these cutting-edge fields.. **The workshop will take place via Zoom at 8:00 PM to 9:15 PM starting on March 20th(07 Chaitra 2080).**","tags":[],"title":"Online Workshop on Data Analysis and Machine Learning with Python","type":"talk"},{"authors":null,"categories":null,"content":"I am thrilled to share my recent participation at the esteemed WCO-ESCAP 6th UNNExT Masterclass on \u0026ldquo;Electronic Single Windows and Data Analytics for Trade Compliance and Facilitation.\u0026rdquo; Held in Seoul, Republic of Korea from February 28th to March 8th, 2024, this event was a pivotal opportunity to contribute to the advancement of international trade practices.\nThroughout the masterclass, I led four pivotal sessions, each offering hands-on experience and practical insights into leveraging data analytics to enhance trade compliance and facilitation:\nData Quality and Cleaning (Hands-on Session): This session provided attendees with practical exercises to address data quality issues effectively, laying a strong foundation for accurate analysis and informed decision-making.\nDATE Algorithm and Risk Management (Hands-on Session): Participants actively engaged in exploring the powerful Lite DATE algorithm and its application in risk management strategies, gaining firsthand experience in utilizing data-driven approaches to enhance trade security.\nLarge Language Models (LLMs): Customs Perspective (Hands-on Session): Attendees delved into the potential of cutting-edge natural language processing technologies, specifically Large Language Models, within the customs domain. Through interactive exercises, participants discovered how these models can revolutionize communication and streamline processes for greater efficiency.\nData Analytics: Use Cases in Nepal Customs: Bringing theory to life, I presented real-world examples of data analytics implementation in Nepal Customs operations, illustrating how data analytics can effectively enhance trade facilitation and compliance in Nepal.\nMy engagement in the masterclass aimed to empower participants with practical skills and insights to navigate the complexities of cross-border trade effectively. By offering hands-on experience and sharing real-world examples, I endeavored to enrich the understanding and capabilities of attendees, equipping them with actionable knowledge for improved trade practices.\nThis masterclass, jointly organized by the World Customs Organization (WCO) and the United Nations Economic and Social Commission for Asia and the Pacific (ESCAP), with the support of various esteemed organizations, attracted participants from over 20 countries across the Asia-Pacific region. Under the theme of \u0026ldquo;Electronic Single Windows and Data Analytics for Trade Compliance and Facilitation,\u0026rdquo; the event provided a comprehensive platform for knowledge exchange and skill development in the realm of international trade.\nFor further details on the masterclass, please visit the UNESCAP website:\rhttps://www.unescap.org/events/2024/wco-escap-6th-unnext-masterclass\n","date":1707382800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1707382800,"objectID":"6ebb4fda72e08f83e6d5275b75660648","permalink":"https://pudasainimohan.com.np/talk/2024_masterclass/","publishdate":"2024-03-10T00:00:00Z","relpermalink":"/talk/2024_masterclass/","section":"talk","summary":"I led data analytics sessions at the WCO-ESCAP trade masterclass in Seoul. Sharing practical tools on data quality, risk assessment,DATE model, and Large Language Models, I empowered customs officials from over 20 countries to improve trade practices through data analysis.","tags":[],"title":"WCO-UNESCAP 6th UNNExT Masterclass","type":"talk"},{"authors":["Mohan Kumar Pudasaini"],"categories":["BACUDA"],"content":"The culmination of my BAnd of CUstoms Data Analytics(BACUDA) scholarship journey today was a momentous occasion, marked by an exquisitely organized closing ceremony anchored by Professor Mitra Ghergherehchi. The esteemed presence of President Ji Beom Yoo of Sungkyunkwan University(SKKU), Director Chulhun Lee of Korea Customs Services(KCS), and Professor Jong Seo Chai of SKKU further elevated the celebration. Their congratulatory speeches eloquently resonated with the program\u0026rsquo;s success, highlighting the significance of our collective achievement.\nAdding to the festivities, Director Taeil Kang of the World Customs Organization graced us with a heartfelt video message, extending his warm congratulations. The BAUDA program has been an unparalleled crucible of transformative experiences and profound learning for me. As the Valedictorian , I was deeply honored to speak about the program and the impact this program has had on our lives.\nValedictorian speech\nReflecting on this journey, I am overflowing with gratitude for the invaluable lessons, connections, and personal growth fostered by the BAUDA program. It has been a privilege to be part of such a prestigious initiative, and I eagerly anticipate applying the knowledge and skills gained here to make a meaningful contribution in my future endeavors.\n","date":1706227200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1706227200,"objectID":"2c579b014fa31e84d693674203011961","permalink":"https://pudasainimohan.com.np/post/bacuda_closing/","publishdate":"2024-01-26T00:00:00Z","relpermalink":"/post/bacuda_closing/","section":"post","summary":"The culmination of my BAnd of CUstoms Data Analytics(BACUDA) scholarship journey today was a momentous occasion, marked by an exquisitely organized closing ceremony anchored by Professor Mitra Ghergherehchi. The esteemed presence of President Ji Beom Yoo of Sungkyunkwan University(SKKU), Director Chulhun Lee of Korea Customs Services(KCS), and Professor Jong Seo Chai of SKKU further elevated the celebration.","tags":["Customs","AI"],"title":"Culmination of the BACUDA Journey: A Celebration of Transformative Learning and Collective Achievement","type":"post"},{"authors":null,"categories":null,"content":"This marks my second venture into the realm of Harmonic code classification for products. In my initial project, I employed the BERT pre-trained model, fine-tuning it with proprietary data to create an effective classification model. However, the winds of change have led me to my latest project, where I harnessed the capabilities of Google Gemini, the forefront of AI innovation.\nGoogle Gemini: Redefining the AI Landscape The emergence of Google Gemini represents a paradigm shift in the era of AI. Unlike its predecessors, this model stands out for its unique ability to seamlessly integrate both textual and visual information. This means that whether you provide a commercial description or a picture of the product, Google Gemini has the prowess to decipher and recommend the most fitting Harmonic code.\nTechnical Insights into Model Architecture From a technical standpoint, I leveraged two key components of the Gemini suite ‚Äì Gemini-Pro-Vision for image analysis and Gemini-Pro for text processing. The synergy between these components allows for a holistic approach to Harmonic code classification. The secret sauce lies not only in the model architecture but also in the meticulous engineering of prompts, ensuring consistent and reliable results with every query.\nThreefold Output for Comprehensive Insights This dynamic model delivers results in three distinct parts, each contributing to a comprehensive understanding of the product:\nRecommended HS Code and Rationale: The primary output is the suggested Harmonic code along with a detailed explanation of the reasoning behind it. This ensures transparency and builds confidence in the accuracy of the classification.\nDescription of the Image: For queries involving product images, Google Gemini provides a detailed description. This not only aids in verification but also serves as an additional layer of information for users.\nOther Possible HS Codes: Acknowledging the nuances of product classification, the model goes beyond a singular recommendation. It provides alternative Harmonic codes, allowing users to explore different categorization possibilities.\nEnsuring Consistency with Prompt Engineering An often-overlooked but crucial aspect of this model is prompt engineering. Through meticulous crafting of prompts, I have ensured that the structure remains consistent, guaranteeing reliable outcomes each time the model is engaged.\nA quick demo and further details have been presented in the following video:\n","date":1703980800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1703980800,"objectID":"d51be354bb9023a719583968ef69d9ce","permalink":"https://pudasainimohan.com.np/project/hs_classification_image/","publishdate":"2023-12-31T00:00:00Z","relpermalink":"/project/hs_classification_image/","section":"project","summary":"HS Code recommendation from image or description of goods","tags":["Deep Learning","NLP"],"title":"HS recommendation from image","type":"project"},{"authors":null,"categories":null,"content":"Despite the waning presence of COVID, CT scan images captured during the pandemic persist as a crucial resource for deep learning practitioners in medical imaging. In my recent Image Classification project, I delved into the realm of multi-class classification using two influential deep learning models, ResNet50 and EfficientNet-B0. In this post, I present a detailed analysis of my recent Image Classification project, where I utilized two popular deep learning models, ResNet50 and EfficientNet-B0, to classify COVID CT scans.\nDataset Overview The dataset consists of a diverse range of CT scans, including:\nHealthy scans: 758 SARS-CoV-2 infected scans: 2,168 Other pulmonary conditions: 1,247\nModels Used ResNet50 Training Accuracy: 94.48% Testing Accuracy: 87.81% EfficientNet-B0 Training Accuracy: 98.95% Testing Accuracy: 92.14% Results and Insights ResNet50 Performance\nResNet50 model achieved an impressive training accuracy of 94.48% and a testing accuracy of 87.81%. This indicates robust learning capabilities and generalization to unseen data. The model excelled in distinguishing between healthy scans, SARS-CoV-2 infected scans, and other pulmonary conditions.\nEfficientNet-B0 Performance\nThe EfficientNet-B0 model outperformed ResNet50, achieving a training accuracy of 98.95% and a testing accuracy of 92.14%. This suggests that the model has a higher capacity to learn intricate patterns and features within the dataset. The enhanced performance could be attributed to the model\u0026rsquo;s architecture, which optimizes both depth and width.\nCode Contribution To foster collaboration and transparency, I have made our Python code publicly available on two prominent platforms:\nGitHub: Here\nKaggle: Here\n","date":1699920000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1699920000,"objectID":"64d6abc4e79ed2d048419dfa35ab134c","permalink":"https://pudasainimohan.com.np/project/covid_classification/","publishdate":"2023-11-14T00:00:00Z","relpermalink":"/project/covid_classification/","section":"project","summary":"Image Classification, ResNet50 vs EfficientNet-B0.","tags":["Deep Learning","Computer Vision"],"title":"COVID CT Scan Classification: ResNet50 vs. EfficientNet-B0","type":"project"},{"authors":null,"categories":null,"content":"In today\u0026rsquo;s digital age, providing quick and accurate information to your website visitors is crucial. Whether you run an e-commerce business or a travel agency, having a chatbot that can assist users with their inquiries can greatly enhance the user experience. In this post, we\u0026rsquo;ll explore the development of a chatbot using the GPT-2 pretrained model to provide Nepal Customs information.\nGPT-2, which stands for \u0026ldquo;Generative Pre-trained Transformer 2,\u0026rdquo; is an advanced language model developed by OpenAI. It\u0026rsquo;s designed to understand and generate human-like text, making it an ideal candidate for developing chatbots and virtual assistants. The GPT-2 pretrained model is trained on a massive amount of text data, making it highly proficient in understanding and generating natural language.\nCreating a chatbot to provide Nepal Customs information can be invaluable for various users, including travelers, importers, and exporters. Accuracy, availability, efficiency and user-friendliness are the major features of the chatbot.\nThe development of the Nepal Customs Information Chatbot relies on deep learning technology. Deep learning is a subset of machine learning that employs artificial neural networks to mimic human decision-making processes. In this case, the GPT-2 pre-trained model utilizes deep learning techniques to understand and generate text based on patterns and context.\nDeep learning technology allows the chatbot to continually improve and adapt to user interactions. It learns from every conversation, making it smarter and more efficient over time.\nTo give you a glimpse of the Nepal Customs Information Chatbot in action, here\u0026rsquo;s a short video demonstrating how it can assist users with their customs-related queries: ","date":1698019200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698019200,"objectID":"d80736a3f12026737b41336afa092bb3","permalink":"https://pudasainimohan.com.np/project/chat_bot/","publishdate":"2023-10-23T00:00:00Z","relpermalink":"/project/chat_bot/","section":"project","summary":"An AI chatbot built on the GPT architecture.","tags":["Deep Learning","NLP"],"title":"Chatbot using GPT-2","type":"project"},{"authors":null,"categories":null,"content":"In today\u0026rsquo;s globalized world, international trade serves as the lifeblood of economic growth. The seamless exchange of goods across borders has never been more crucial, and at the heart of this intricate process lies the Harmonized System (HS). HS codes, standardized numerical identifiers assigned to products, play a pivotal role in streamlining the import and export of goods worldwide. This robust system was created by the World Customs Organization (WCO) to provide a common framework for classifying products, ensuring consistency and efficiency in global trade. However, accurately classifying products can be a daunting challenge for both customs authorities and businesses alike, given the complexity and scope of modern international commerce.\nBut fear not, for the advent of artificial intelligence (AI) and machine learning has ushered in a new era of efficiency and precision in the realm of HS code classification. In this post, we introduce you to our revolutionary HS code classification model, powered by the BERT architecture, which promises to make the process smoother than ever before.\nUnderstanding the HS Classification Model Our model is designed to classify products based on user-provided descriptions. It offers a breakthrough solution by presenting the top three most likely HS codes for a given product description. What\u0026rsquo;s even more remarkable is that we\u0026rsquo;ve tailored this model to the specific context of Nepal, a country that uses an 8-digit level HS code classification system.\nAdditionally, our model is versatile and user-friendly. It caters to a wider audience by accepting product descriptions in three languages: English, Nepali, and Romanized Nepali text. This multilingual capability ensures that users from different linguistic backgrounds can easily utilize the model to classify products, making international trade even more accessible.\nThe Power of BERT and the Fight Against Fraud Our preliminary version of the model is constructed using a deep neural network based on the BERT architecture. BERT (Bidirectional Encoder Representations from Transformers) is renowned for its natural language processing capabilities, which enable it to understand and interpret textual data more comprehensively. By harnessing the power of BERT, our HS code classification model not only simplifies the classification process but also enhances its accuracy.\nBeyond simply improving efficiency, our model also serves as a valuable tool in mitigating fraudulent activities. Accurate product classification is a fundamental step in preventing goods from being misclassified, which can lead to underpayment of customs duties and taxes. By reducing the probability of fraudulent activities, our model contributes to the integrity of international trade.\nIn conclusion, our HS code classification model represents a significant step forward in the world of international trade. Its ability to efficiently and accurately classify products, its multilingual support, and its application in Nepal\u0026rsquo;s 8-digit HS code context make it a valuable asset for businesses, customs authorities, and trade professionals. As we continue to refine and expand upon this model, it has the potential to revolutionize the way we approach global trade.\nTo see our model in action and learn more about its features, check out the following video:\nStay tuned for more updates on our journey to enhance international trade through the power of AI and HS code classification.\n","date":1697932800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1697932800,"objectID":"08d7b7c59168f990f513767cbdf2fe4b","permalink":"https://pudasainimohan.com.np/project/hs_classification/","publishdate":"2023-10-22T00:00:00Z","relpermalink":"/project/hs_classification/","section":"project","summary":"AI-Powered HS Code Classification Model","tags":["Deep Learning","NLP"],"title":"HS Code Classification Model","type":"project"},{"authors":["Mohan Kumar Pudasaini"],"categories":["BACUDA"],"content":"I\u0026rsquo;m excited to update you on my ongoing journey through the WCO BACUDA Scholarship Programme, which commenced on September 4, 2023. As we delve deeper into this program, I\u0026rsquo;ve been gaining valuable insights and knowledge, and I can\u0026rsquo;t wait to share some highlights with you. This program comprises 12 globally selected participants and is hosted by Sungkyunkwan University (SKKU).\nInspiring Opening Ceremony: The opening ceremony set the stage for what promises to be an exciting journey. We were welcomed by esteemed leaders in the field, including Dr. Taeil KANG, Director of the Capacity Building Directorate at the WCO, Dr. Ji Beom Yoo, President of SKKU, Professor Jong Seo Chai of SKKU, and Mr. Chul Hoon Lee, Head of the International Cooperation Division of the Korea Customs Service. Their words of encouragement have fueled our passion for learning and applying data analytics in the Customs domain.\nA Dynamic Learning Experience: The WCO BACUDA Scholarship Programme is proving to be a dynamic and enriching learning experience. Alongside eleven other Customs officials from diverse regions, I am actively engaged in expanding my understanding of data analytics and its profound impact on Customs operations. As a data analyst, I have been working with limited knowledge, and this program will expand my expertise into new areas of modern AI, such as Deep Learning, Blockchain technology, and network analysis.\nExploring Cutting-Edge Topics: Our curriculum is a treasure trove of knowledge, covering a wide range of data analytics topics, from the basics to advanced concepts. We\u0026rsquo;re diving into areas like Big Data Analytics, Machine Learning, Blockchain, and the latest algorithms developed by the BACUDA expert group. These topics are opening new horizons for me and my fellow participants. Hands-on practice with the UNIPASS system expert and the Big Data Academy will add even more excitement and learning.\nThe Group Project Challenge : One of the most thrilling aspects of this program is our group project challenge. Learning is one thing, but applying learning in the real field is another thing. Over the course of five months, along with theoretical learning, we will develop AI algorithms to address the specific challenges faced by our Customs administrations. It\u0026rsquo;s a hands-on opportunity to apply what we\u0026rsquo;ve learned and drive meaningful change.\nBuilding a Global Network: Being part of a diverse cohort from countries such as Argentina, Bangladesh, Brazil, Ghana, Maldives, Mauritius, Mongolia, Namibia, Nepal, Sri Lanka, Zimbabwe, and Madagascar has been eye-opening. We\u0026rsquo;re not only learning from experts but also from each other, exchanging insights and perspectives from around the world.\nAs I continue this incredible journey of learning and growth, I want to express my profound gratitude for the unwavering support I\u0026rsquo;ve received from Nepal Customs, WCO, and CCF Korea for their generous funding, without which this opportunity would not have been possible, as well as my cherished friends and family who have been my pillars of strength throughout. I eagerly anticipate sharing more profound insights and transformative experiences as I progress further in this enriching program. Lastly, I extend my heartfelt appreciation to Professor Mitra GHERGHEREHCHI, and her dedicated team for their remarkable coordination and warm hospitality. As we embark on the next phase of this journey after completing our first week, it is evident that the professors\u0026rsquo; approachability, their vast knowledge in the realm of data science, the enticing curriculum, and the serene atmosphere of SKKU will undoubtedly propel us towards new heights of knowledge and accomplishment. Many exciting opportunities await on this path of discovery, and I am truly grateful for the chance to be a part of it.\nReleted news on WCO website:\rOpening of the Second WCO BACUDA Scholarship Programme at Sung Kyun Kwan University (SKKU)\n","date":1694217600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1694217600,"objectID":"8b2988d19d4b9772eb625f94f777741e","permalink":"https://pudasainimohan.com.np/post/bacuda_opening/","publishdate":"2023-09-09T00:00:00Z","relpermalink":"/post/bacuda_opening/","section":"post","summary":"I\u0026rsquo;m excited to update you on my ongoing journey through the WCO BACUDA Scholarship Programme, which commenced on September 4, 2023. As we delve deeper into this program, I\u0026rsquo;ve been gaining valuable insights and knowledge, and I can\u0026rsquo;t wait to share some highlights with you.","tags":["Customs","AI"],"title":"Expanding Horizons: My Journey of Learning with the WCO BACUDA Scholarship Programme","type":"post"},{"authors":null,"categories":null,"content":"As a Customs official at Nepal Customs with a specialization in risk management, my quest for knowledge and innovation led me to an enriching experience - the course titled \u0026ldquo;Artificial Intelligence in Border Management.\u0026rdquo; Offered by the University of Victoria (Canada) in partnership with Border in Globalization(BiG) Lab, this course promised to delve into the realm of Artificial Intelligence (AI) and its applications within the intricate landscape of border control and customs management. Over the span of four days, I was immersed in a world of insights, discussions, and hands-on learning that expanded my horizons.\nDay 1: Laying the Foundation\nThe course commenced with a captivating session led by Thomas Cantens (World Customs organization), a brilliant mind who explored the historical and anthropological significance of customs and mathematics. This intriguing perspective shed light on how these seemingly unrelated fields have converged over time to play a pivotal role in border management. The subsequent sessions, steered by Peter Swartz(Co-founder, https://altana.ai/) introduced us to the fundamental concepts of AI and machine learning (ML). We explored the basic definitions, mechanisms, and the potential they hold for revolutionizing border control. The day continued with an exploration of knowledge graphs, offering a glimpse into how intricate network representations of the world can be leveraged for modeling trade. The practical aspect of Day 1 allowed us to apply our nascent understanding to real-world scenarios.\nDay 2: Targeting Illicit Activities with Precision\nThe second day was dedicated to understanding how AI and ML can be employed to target various forms of illicit activities. Peter Swartz guided us through the intricacies of employing AI to combat smuggling ‚Äì whether it\u0026rsquo;s narcotics, weapons, or counterfeits. The session expanded to AI\u0026rsquo;s role in targeting revenue evasion, a critical aspect of trade regulation. The most intriguing discussions revolved around using AI for trade facilitation while maintaining the enforcement of laws and regulations. The day\u0026rsquo;s practical exercises honed our ability to devise AI-driven strategies against illicit activities.\nDay 3: Real-world Applications and Ethical Considerations\nRim Khazall from the Canada Services and Borders Agency shared invaluable insights into the real-world applications of AI. Reflecting on practical use cases added a layer of authenticity to our learning experience. The day further delved into the deployment of AI/ML systems, tackling critical issues of privacy and security. We also grappled with the ethical dimensions of AI, exploring the defensibility and explainability of the models we were learning to create.\nDay 4: Crafting Policies for the Future\nAs the course neared its conclusion, Claude Beaupres guided us through the realm of policy making, emphasizing the importance of crafting well-informed policy papers and briefs. This session bridged the technical knowledge gained throughout the course with the practical application in the real world. The day concluded with a synthesis of our learning journey through practical exercises and the opportunity to compose a paper that encapsulated our insights and learnings from the course.\nCompleting the \u0026ldquo;Artificial Intelligence in Border Management\u0026rdquo; course has been an enlightening journey that has significantly enriched my perspective as a data science practitioner. This course went beyond the technical aspects of AI and delved deep into its transformative potential within the realm of border control. Not only did it equip me with a comprehensive understanding of the various AI technologies applicable in this context, but it also honed my ability to critically assess the ethical, security, and policy dimensions intrinsic to their deployment.\n","date":1690185600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1690185600,"objectID":"797d173bf8e28252b281ad754a3300de","permalink":"https://pudasainimohan.com.np/talk/2023_uvic/","publishdate":"2023-02-14T00:00:00Z","relpermalink":"/talk/2023_uvic/","section":"talk","summary":"Artificial Intelligence in Border Management offered by the University of Victoria in partnership with Border in Globalization. The course spans four days and delves into the applications of AI in border control and customs management. It explores foundational AI concepts, targeting illicit activities, real-world applications, ethical considerations, and policy crafting. The course blends theoretical insights with practical exercises, providing participants with a transformative perspective on AI potential in border management","tags":[],"title":" Navigating the \"Artificial Intelligence in Border Management\" Course","type":"talk"},{"authors":null,"categories":null,"content":"The United States Department of Energy, National Nuclear Security Administration conducted a three-day event in Kathmandu, Nepal called \u0026ldquo;Nepal Border Security Workshop\u0026rdquo;. The event was held from March 23 to 25, 2023, and was attended by personnel from various organizations such as Nepal Customs, Nepali Army, Nepal Police, Armed Police Force, and border experts. The primary focus of the event was on nuclear and radioactive materials detection, location, and identification. Experts from the United States demonstrated their expertise in nuclear and radioactive materials technology, various devices used for detection, location, and identification of such materials, and best practices from other countries. Scenario-based discussions were conducted to address the challenges and issues related to nuclear smuggling, detection, and deterrence.\nThe \u0026ldquo;Nepal Border Security Workshop\u0026rdquo; was a valuable opportunity for personnel from different organizations to come together and discuss critical issues related to border security, particularly in the detection and prevention of nuclear and radioactive materials smuggling.\n","date":1679562000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1679562000,"objectID":"a18b5a36fddaccfe986e02cd3ef1ce40","permalink":"https://pudasainimohan.com.np/talk/2023_radioactive/","publishdate":"2023-03-29T00:00:00Z","relpermalink":"/talk/2023_radioactive/","section":"talk","summary":"US Department of Energy organized a 3-day \"Nepal Border Security Workshop\" in Kathmandu, Nepal. The event focused on detecting nuclear and radioactive materials, and experts shared their best practices. Personnel from various organizations attended, providing a platform to discuss critical border security issues.","tags":[],"title":"US Department of Energy Conducted:Nepal Border Security Workshop","type":"talk"},{"authors":null,"categories":null,"content":"‚ÄÉFee Rs 2000 only\nSyllabus‚ÄÉClick here\nFor the video of the Opening session, click here..\nPython has become a critical tool for data scientists and analysts, offering a vast range of capabilities for analyzing and visualizing data. Whether you are a beginner or looking to enhance your existing skills in python, then this workshop is for you. This workshop is designed for individuals from a variety of backgrounds who have an interest in data analysis and visualization and want to learn the basics of Python programming without prior coding knowledge. The workshop is specifically targeted towards:\nData Analysts/Data scientist: Currently working with different software and looking to expand their skills by learning Python for data analysis. Students: Interested in starting a career in data analysis or data science and want to learn the basics of data analysis using Python. Researchers and Scholars: Who need to analyze data for their thesis, research projects, or academic papers. Educators and Teachers: Who want to incorporate data analysis and visualization into their curriculum or learn Python as a tool for teaching data science. Business Professionals: Who need to analyze data for their work and want to learn a powerful and versatile programming language for data analysis. By the end of the workshop, you will have a solid foundation in Python programming and the ability to use Python for various data analysis and visualization tasks. With hands-on experience using Jupyter Notebook and the various libraries used in the workshop, you will be ready to tackle real-world data analysis projects. To view the full syllabus, please Click here .\nDon\u0026rsquo;t miss out on this opportunity to learn the most in-demand skills in the data analysis field. With a low fee, this comprehensive beginner\u0026rsquo;s workshop offers exceptional value for anyone looking to become a proficient Python data analyst. Enroll now to take your skills to the next level!\n","date":1677441600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677441600,"objectID":"8a2808734768016f10d182eed59fc25f","permalink":"https://pudasainimohan.com.np/talk/basic_data_analysis/","publishdate":"2023-02-14T00:00:00Z","relpermalink":"/talk/basic_data_analysis/","section":"talk","summary":"This 15-day workshop is designed for individuals interested in data analysis and visualization who want to learn the basics of Python programming. It covers the fundamentals of data analysis using Python and is suitable for individuals with varying levels of experience, including complete beginners. **The workshop will take place via Zoom at 8:00 PM to 9:15 PM starting on February 26th(14 Fagun 2079).**","tags":[],"title":"Online Workshop on Data Analysis with Python","type":"talk"},{"authors":["Mohan Kumar Pudasaini"],"categories":["Python"],"content":"In Python, a tuple is a data structure that is similar to a list, but with a key difference: tuples are immutable. This means that once a tuple is created, its elements cannot be changed. Tuples are often used to store collections of related values that should not be modified, such as days of the week, or dates on a calendar. In this article, we will explore tuples in more detail, including how to construct them, some basic tuple methods. We will also discuss the concept of immutability, which is a fundamental feature of tuples that sets them apart from other data structures\nConstructing Tuples There are several ways to create a tuple. The most common way is to enclose a sequence of values in parentheses, separated by commas. For example, you can create a tuple that stores the names of the months like this:\n# Create a tuple\rmonths = (\u0026quot;January\u0026quot;, \u0026quot;February\u0026quot;, \u0026quot;March\u0026quot;, \u0026quot;April\u0026quot;, \u0026quot;May\u0026quot;, \u0026quot;June\u0026quot;, \u0026quot;July\u0026quot;, \u0026quot;August\u0026quot;, \u0026quot;September\u0026quot;, \u0026quot;October\u0026quot;, \u0026quot;November\u0026quot;, \u0026quot;December\u0026quot;)\rmonths\r('January',\r'February',\r'March',\r'April',\r'May',\r'June',\r'July',\r'August',\r'September',\r'October',\r'November',\r'December')\rYou can also create a tuple using the built-in tuple() function, which takes an iterable (such as a list or another tuple) and converts it into a tuple. For example:\nmy_list = [1, 2, 3]\rmy_tuple = tuple(my_list)\rmy_tuple\r(1, 2, 3)\rIf you have a tuple containing the months of the year, you can use len() to find out how many months are in the tuple:\nlen(months)\r12\rOne of the benefits of tuples in Python is that they can contain elements of different types. For example, you can create a tuple that stores both integers and strings:\nmy_tuple = (1, \u0026quot;hello\u0026quot;, 3.14)\rmy_tuple\r(1, 'hello', 3.14)\rIndexing and slicing tuples is similar to indexing and slicing lists in Python. To access a single element of a tuple, you can use square brackets and the index of the element you want to access. In Python, indices start at 0, so the first element of a tuple has an index of 0, the second element has an index of 1, and so on. For example:\nmonths[0]\r'January'\rmonths[::2]\r('January', 'March', 'May', 'July', 'September', 'November')\rBasic Tuple Methods Tuples have built-in methods, but not as many as lists do. Let\u0026rsquo;s look at two of them:\nYou can use the .index() method to find the index of a specific value in a tuple. If the value is found in the tuple, the method returns the index of the first occurrence of the value. If the value is not found, the method raises a ValueError\nmonths.index(\u0026quot;January\u0026quot;)\r0\rmonths.index(\u0026quot;Nepal\u0026quot;)\r---------------------------------------------------------------------------\rValueError Traceback (most recent call last)\rCell In[10], line 1\r----\u0026gt; 1 months.index(\u0026quot;Nepal\u0026quot;)\rValueError: tuple.index(x): x not in tuple\rYou can use the .count() method to count the number of times a specific value appears in a tuple. This method returns an integer that represents the number of occurrences of the value in the tuple. for example\nt=(1,2,3,4,2,1,2,4)\rt.count(2)\r3\rImmutability Tuple can\u0026rsquo;t be stressed enough thay are immutable. To drive that point home, let\u0026rsquo;s take an example and try to replace \u0026ldquo;january\u0026rdquo; by \u0026ldquo;Jan\u0026rdquo;\nmonths[0]= 'Jan'\r---------------------------------------------------------------------------\rTypeError Traceback (most recent call last)\rCell In[13], line 1\r----\u0026gt; 1 months[0]= 'Jan'\rTypeError: 'tuple' object does not support item assignment\rBecause of this immutability, tuples can\u0026rsquo;t grow. Once a tuple is made we can not add to it.\nmonths.append('Baishakh')\r---------------------------------------------------------------------------\rAttributeError Traceback (most recent call last)\rCell In[14], line 1\r----\u0026gt; 1 months.append('Baishakh')\rAttributeError: 'tuple' object has no attribute 'append'\rIn summary, Tuples are immutable data structures in Python, similar to lists but with key differences. They can be constructed using parentheses, and can contain multiple data types. Tuples support basic methods like indexing, slicing, counting, and returning the length of the tuple. The immutability of tuples makes them useful for representing constant values, while their compact size and faster processing speed make them a good choice for certain types of data.\n","date":1676592000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1676592000,"objectID":"8cd8d54655913ae255906aaf2909dda7","permalink":"https://pudasainimohan.com.np/post/intro_tuple/","publishdate":"2023-02-17T00:00:00Z","relpermalink":"/post/intro_tuple/","section":"post","summary":"Tuples in Python, a data structure that is similar to lists but immutable. It covers constructing tuples, indexing and slicing them, and basic tuple methods. The concept of immutability is discussed, highlighting its fundamental feature. Tuples are useful for storing related values that should not be modified and are faster and more compact than lists.","tags":["Python Basics","Tuple","Data Science"],"title":"An Introduction to Tuples in Python","type":"post"},{"authors":["Mohan Kumar Pudasaini"],"categories":["Python"],"content":"Python dictionaries provide a powerful way to store and organize data as key-value pairs, which is known as mapping. Unlike sequences that store objects by their relative position, mappings use a unique key to store objects. This key-based approach makes Python dictionaries incredibly versatile and suitable for representing a wide range of real-world scenarios. It\u0026rsquo;s important to note that since mappings are not defined by order, Python dictionaries won\u0026rsquo;t retain the order of objects. In this article, we\u0026rsquo;ll delve into the basics of Python dictionaries, including how to construct a dictionary, access objects within it, create nested dictionaries, and use some of the basic dictionary methods.\nConstructing a Dictionary In Python, you can construct a dictionary using a set of key-value pairs enclosed in curly braces {}. Each key-value pair is separated by a colon, and the pairs are separated by commas. Here\u0026rsquo;s an example of a simple dictionary\nfruit = {'apple': 2, 'banana': 3, 'orange': 5}\rIn this example, the keys are strings (\u0026lsquo;apple\u0026rsquo;, \u0026lsquo;banana\u0026rsquo;, \u0026lsquo;orange\u0026rsquo;) and the values are integers (2, 3, 5). You can access the values in the dictionary by using the keys as indices. For example, fruit[\u0026lsquo;apple\u0026rsquo;] would return the value 2.\nfruit['apple']\r2\rIts important to note that dictionaries are very flexible in the data types they can hold. For example:\nfruit = {'name': 'apple','color': 'red','vitamins': ['A', 'C']}\rhere Vitamin contains the list and to access the value associated with a particular key in a dictionary, you can use square bracket notation with the key inside the brackets. For example, to access the value associated with the \u0026lsquo;vitamins\u0026rsquo; key in the \u0026lsquo;fruit\u0026rsquo; dictionary, you can use the following code: fruit[\u0026lsquo;vitamins\u0026rsquo;]\nfruit['vitamins']\r['A', 'C']\rThis will return the value associated with the \u0026lsquo;vitamins\u0026rsquo; key, which in this case is the list [\u0026lsquo;A\u0026rsquo;, \u0026lsquo;C\u0026rsquo;].Once you have the list, you can perform various operations on it. For example, you can access individual elements in the list using their index, like this: fruit[\u0026lsquo;vitamins\u0026rsquo;][0]. This would return the string \u0026lsquo;A\u0026rsquo;, which is the first item in the list.\nfruit['vitamins'][0]\r'A'\ryou can also call methods on that value, for example\nfruit['vitamins'][0].lower()\r'a'\rThis would return the string \u0026lsquo;a\u0026rsquo;, since the lower() method converts the string \u0026lsquo;A\u0026rsquo; to lowercase.\nyou can modifiy the element of the dictionary. for example\nfruit['vitamins'].append('B')\rfruit\r{'name': 'apple', 'color': 'red', 'vitamins': ['A', 'C', 'B']}\rhere, new element \u0026lsquo;B\u0026rsquo; to the vitamins has been added.\nWe can also create keys by assignment. For instance if we started off with an empty dictionary, we could continually add to it:\nCreate a new dictionary\rd = {}\rCreate a new key through assignment\rd['animal'] = 'Dog'\rCan do this with any object\rd['answer'] = 42\rd\r{'animal': 'Dog', 'answer': 42}\rNesting with Dictionaries By now, you might be realizing how flexible and powerful Python is, especially when it comes to nesting objects and calling methods on them. To further illustrate this, consider the following example of a dictionary nested inside another dictionary\nDictionary nested inside a dictionary nested inside a dictionary\rd = {'key1':{'nestkey':{'subnestkey':'value'}}}\rWow! That\u0026rsquo;s a quite the inception of dictionaries! Let\u0026rsquo;s see how we can grab that value:\nKeep calling the keys\rd['key1']['nestkey']['subnestkey']\r'value'\rA few Dictionary Methods There are a few methods we can call on a dictionary. Let\u0026rsquo;s get a quick introduction to a few of them:\nCreate a typical dictionary\rd = {'key1':1,'key2':2,'key3':3}\rMethod to return a list of all keys d.keys()\rdict_keys(['key1', 'key2', 'key3'])\rMethod to grab all values\rd.values()\rdict_values([1, 2, 3])\ritems() emthod return tuples of all items , we will discuss about tuple in next article.\nd.items()\rdict_items([('key1', 1), ('key2', 2), ('key3', 3)])\rIn summary, dictionaries in Python are a powerful and flexible data structure that allow you to store and organize data in key-value pairs. Dictionaries are unordered collections of objects, where each object is identified by a unique key. They are mutable, which means you can modify their contents by adding, removing, or updating key-value pairs. Additionally, the keys in a dictionary are unique, which means you cannot have multiple entries with the same key.\n","date":1676419200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1676419200,"objectID":"f3f08c79c70aa56bc996b43d27f5a968","permalink":"https://pudasainimohan.com.np/post/python_dictionary/","publishdate":"2023-02-15T00:00:00Z","relpermalink":"/post/python_dictionary/","section":"post","summary":"Python dictionaries are powerful and flexible data structures that store data in key-value pairs. Dictionaries are unordered, mutable, and keys are unique. This article explains how to construct, access, and modify dictionaries, as well as how to create nested dictionaries and use some of the basic dictionary methods.","tags":["Python Basics","Dictionary","Data Science"],"title":"Python Dictionaries: Key-Value Pair Mapping","type":"post"},{"authors":["Mohan Kumar Pudasaini"],"categories":["Python"],"content":"A list in Python is a collection of items, which can be of any data type, including numbers, strings, and other objects. Lists are defined using square brackets [] and the items are separated by commas. For example:\nfruits = [\u0026quot;apple\u0026quot;, \u0026quot;banana\u0026quot;, \u0026quot;cherry\u0026quot;]\rnumbers = [1, 2, 3, 4, 5]\rmy_list = ['Ram',23,100.232,'Book',True]\rJust like strings, the len() function will tell you how many items are in the sequence of the list.\nlen(my_list)\r5\rIndexing and Slicing Indexing and slicing are two important concepts in Python when it comes to working with lists.\nIndexing refers to accessing individual elements of a list. The elements of a list are stored in a specific order and can be retrieved using their index, which is an integer that starts from 0. For example, consider the following list\nmy_list = ['Ram',23,100.232,'Book',True]\rYou can access the first element of the list using the following syntax:\nmy_list[0]\r'Ram'\rSlicing, on the other hand, allows you to retrieve a portion of the list. The syntax for slicing is list[start:stop:step] , where start is the starting index (inclusive), stop is the ending index (exclusive), and step is the increment between elements.\nHere\u0026rsquo;s an example of slicing a list:\nmy_list[1:]\r['Ram', 100.232, True]\rhere, The expression my_list[1:] is a slice of the list that starts from the index 1 and goes until the end of the list. In other words, it returns all the elements of the list except for the first one\nmy_list[::2]\r['Ram', 100.232, True]\rThe expression my_list[::2] is a slice of the list that starts from the first element (index 0) and goes until the end of the list, taking steps of 2. In other words, it returns every other element of the list, starting from the first one\nConcatinating list You can concatenate two or more lists using the + operator. The + operator creates a new list that is the combination of the elements from the original lists.\nmy_list + ['new item']\r['Ram', 23, 100.232, 'Book', True, 'new item']\rNote: This doesn\u0026rsquo;t actually change the original list!\nmy_list\r['Ram', 23, 100.232, 'Book', True]\rYou would have to reassign the list to make the change permanent.\nmy_list = my_list + ['new1']\rmy_list\r['Ram', 23, 100.232, 'Book', True, 'new1']\rThe * operator can be utilized for duplication, similar to strings,for example\nmy_list * 2\r['Ram',\r23,\r100.232,\r'Book',\rTrue,\r'add new item permanently',\r'Ram',\r23,\r100.232,\r'Book',\rTrue,\r'add new item permanently']\rBasic List Methods There are several built-in methods in Python that you can use to manipulate lists. Here are some of the most commonly used methods. append(element) - Adds an element to the end of the list\nmy_list = [1,2,3]\rmy_list.append(4)\rprint(list)\r[1, 2, 3, 4]\rinsert(index, element) - Inserts an element at a specific position in the list\nmy_list.insert(0,\u0026quot;First\u0026quot;)\rprint(my_list)\r['First', 1, 2, 3, 4]\rremove(element) - Removes the first occurrence of an element from the list.\nmy_list.remove(\u0026quot;First\u0026quot;)\rprint(list)\r[1, 2, 3, 4]\rpop(index) - Removes the element at a specific position in the list and returns it.\nmy_list.pop(0)\rprint(my_list)\r[2, 3, 4]\rsort() - Sorts the elements of the list in ascending order.\nmy_list.sort(reverse=True)\rprint(my_list)\r[4, 3, 2]\rreverse() - Reverses the order of the elements in the list\nmy_list.reverse()\rprint(my_list)\r[2, 3, 4]\rextend(list) - Adds the elements of one list to the end of another list.\nmy_list1 =[\u0026quot;Book\u0026quot;,\u0026quot;Copy\u0026quot;]\rmy_list.append(my_list1)\rprint(my_list)\r[2, 3, 4, ['Book', 'Copy']]\rmy_list.extend(my_list1)\rprint(my_list)\r[2, 3, 4, ['Book', 'Copy'], 'Book', 'Copy']\rThe difference between extend() and append() is that the extend method modifies the original list my_list, adding the elements of my_list1 to the end of it. On the other hand, the append() methods creates a new list that contains the elements of both my_list and my_list1.\nNesting Lists The term \u0026ldquo;nesting lists\u0026rdquo; refers to creating a list that contains other lists as its elements. This can be useful when you want to group related items together\nLet\u0026rsquo;s see how this works!\nLet's make three lists\rlst_1=[1,2,3]\rlst_2=[4,5,6]\rlst_3=[7,8,9]\rMake a list of lists to form a combined_list\rcombined_list = [lst_1,lst_2,lst_3]\rcombined_list\r[[1, 2, 3], [4, 5, 6], [7, 8, 9]]\rWe can again use indexing to access elements, but now there are two levels for the index. The items in the combined_list object, and then the items inside that list!\naccess first item in combined_list object\rcombined_list[0]\r[1, 2, 3]\raccess first item of the first item in the combined_list object\rmatrix[0][0]\r1\r","date":1676246400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1676246400,"objectID":"baf83b5e3070dfef88c3a44fa58c620b","permalink":"https://pudasainimohan.com.np/post/python_list/","publishdate":"2023-02-13T00:00:00Z","relpermalink":"/post/python_list/","section":"post","summary":"comprehensive overview of working with lists in Python, including concepts like defining lists, accessing elements through indexing and slicing, concatenating lists, using basic list methods like append, insert, remove, pop, sort, and more.","tags":["Python Basics","List","Data Science"],"title":"Working with Lists in Python","type":"post"},{"authors":["Mohan Kumar Pudasaini"],"categories":["Python"],"content":"String formatting is a technique used to combine an existing string with other values to create a new string. This process is important in many programming languages, including Python, as it enables developers to create dynamic strings that can change based on different inputs. In data science, string formatting is particularly useful as it allows data scientists to present results in a clear and readable format, streamline their workflows, and effectively communicate their findings. With string formatting, data scientists can process, analyze, and visualize large amounts of data with ease. In this article, we will provide a comprehensive overview of string formatting in Python and explore the various methods available to help you get started with this important technique.\nThere are three ways to perform string formatting.\nPlaceholder Method using % character .format() Method f-strings Method Formatting with placeholders In Python, there are several placeholders that can be utilized for string formatting purposes. These placeholders include %s, %r, and %d. lets\u0026rsquo; look some example of each placeholder:\nprint(\u0026quot;I'm going to insert %s into the code.\u0026quot; %'new text')\rI'm going to insert new text into the code.\rIn this example, The string \u0026ldquo;I\u0026rsquo;m going to insert %s into the code.\u0026rdquo; serves as a template and includes a placeholder, represented by the %s. This placeholder indicates where a value should be inserted into the string.\nThe value that is inserted into the string is \u0026rsquo;new text\u0026rsquo;, which is specified after the % operator. When the code is executed, the output is the string \u0026ldquo;I\u0026rsquo;m going to insert new text into the code.\u0026rdquo;, which shows how the value of \u0026rsquo;new text\u0026rsquo; has been incorporated into the template string.\nprint(\u0026quot;I'm going to insert %s information here, and %s data here.\u0026quot; %('critical','additional'))\rI'm going to insert critical information here, and additional data here.\rIn this example, the placeholders %s are used to dynamically insert the values \u0026lsquo;critical\u0026rsquo; and \u0026lsquo;additional\u0026rsquo; into the string. The % operator is followed by a tuple of values that are inserted into the corresponding placeholders in the string.\na, b = 'important', 'updated'\rprint(\u0026quot;I'm going to insert %s information here, and %s data here.\u0026quot; %(a,b))\rI'm going to insert important information here, and updated data here.\rhere, the variables a and b are assigned the values \u0026lsquo;important\u0026rsquo; and \u0026lsquo;updated\u0026rsquo; respectively. These variables are then used in the string formatting expression to insert the values into the corresponding placeholders in the string. This demonstrates that the values assigned to the variables can be changed easily, making the output dynamic and flexible.\nIt should be noted that two methods %s and %r convert any python object to a string using two separate methods: str() and repr(). We will learn more about these functions on upcomming posts, but you should note that %r and repr() deliver the string representation of the object, including quotation marks and any escape characters. for example:\nprint('My name is %s.' %'Mohan')\rprint('My name is %r.' %'Mohan')\rMy name is Mohan.\rMy name is 'Mohan'.\rIn this example, the string \u0026lsquo;Mohan\u0026rsquo; is being inserted into the sentence \u0026lsquo;My name is %s.\u0026rsquo; and \u0026lsquo;My name is %r.\u0026rsquo; using string formatting.\nThe %s placeholder is a placeholder for a string, and the % symbol followed by \u0026lsquo;Mohan\u0026rsquo; in parentheses is used to replace the %s placeholder with the string \u0026lsquo;Mohan\u0026rsquo;. similarly, The %r placeholder is a placeholder for a string representation of an object, which can be useful for debugging purposes. In this case, the string representation of the string \u0026lsquo;Mohan\u0026rsquo; is \u0026lsquo;Mohan\u0026rsquo;, so the %r placeholder is replaced with \u0026lsquo;Mohan\u0026rsquo;. let\u0026rsquo;s look another example of %r and %s\nwords = 'catch\\nand\\nrelease'\rprint('Remember to %s.' % ('catch\\nand\\nrelease'))\rprint('Remember to %r.' % ('catch\\nand\\nrelease'))\rRemember to catch\rand\rrelease.\rRemember to 'catch\\nand\\nrelease'.\rhere, first line\u0026rsquo;s result comes in three lines and the second line\u0026rsquo;s result comes in one line is due to the difference between %s and %r.\n%s is used to insert a string representation of an object into a string, which means that any newline characters (\\n) in the string representation will be displayed as separate lines in the output.\nOn the other hand, %r is used to insert a raw, string representation of an object into a string, which includes any escape sequences such as newline characters.\nThe %s operator converts whatever it sees into a string, including integers and floats. The %d operator converts numbers to integers first, without rounding. Note the difference below:\nprint('I wrote %s programs today.' %3.75)\rprint('I wrote %d programs today.' %3.75) I wrote 3.75 programs today.\rI wrote 3 programs today.\rPadding and Precision of Floating Point Numbers In Python, you can control the display of floating-point numbers by using the % operator in combination with placeholder specifications. These placeholder specifications allow you to specify the number of decimal places to display, as well as the width of the output field. To control the precision of floating-point numbers, you can use the . character followed by the number of decimal places you want to display\nprint('Floating point numbers: %5.2f' %(13.144))\rFloating point numbers: 13.14\rFloating point numbers use the format %5.2f. Here, 5 would be the minimum number of characters the string should contain; these may be padded with whitespace if the entire number does not have this many digits. Next to this, .2f stands for two digits to show past the decimal point.\nprint('Floating point numbers: %.0f' %(1300.144))\rFloating point numbers: 1300\rFormatting with the .format() method A better way to format objects into your strings for print statements is with the string .format() method. The syntax is:\n'String here {} then also {}'.format('something1','something2')\rFor example:\nprint('This is a string with an {}'.format('insert'))\rThis is a string with an insert\rThe .format() method has several advantages over the %s placeholder method: 1. Inserted objects can be called by index position: print('The {2} {1} {0}'.format('fox','brown','quick'))\rThe quick brown fox\r2. Inserted objects can be assigned keywords: print('First Object: {a}, Second Object: {b}, Third Object: {c}'.format(a=1,b='Two',c=12.3))\rFirst Object: 1, Second Object: Two, Third Object: 12.3\r3. Inserted objects can be reused, avoiding duplication: print('I %s you more every day, and my %s for you will never fade.' %('love','love'))\r#Vs\rprint('I {w} you more every day, and my {w} for you will never fade.'.format(w='love'))\rI love you more every day, and my love for you will never fade.\rI love you more every day, and my love for you will never fade.\rAlignment, padding and precision with .format() Within the curly braces you can assign field lengths, left/right alignments, rounding parameters and more\nprint('{0:8} | {1:9}'.format('Fruit', 'Quantity'))\rprint('{0:8} | {1:20}'.format('Apples', 3.))\rprint('{0:8} | {1:9}'.format('Oranges', 10))\rFruit | Quantity Apples | 3.0\rOranges | 10\rBy default, .format() aligns text to the left, numbers to the right. You can pass an optional \u0026lt;,^, or \u0026gt; to set a left, center or right alignment:\nprint('{0:\u0026lt;8} | {1:^8} | {2:\u0026gt;8}'.format('Left','Center','Right'))\rprint('{0:\u0026lt;8} | {1:^8} | {2:\u0026gt;8}'.format(11,22,33))\rLeft | Center | Right\r11 | 22 | 33\rYou can precede the aligment operator with a padding character\nprint('{0:=\u0026lt;8} | {1:-^8} | {2:.\u0026gt;8}'.format('Left','Center','Right'))\rprint('{0:=\u0026lt;8} | {1:-^8} | {2:.\u0026gt;8}'.format(11,22,33))\rLeft==== | -Center- | ...Right\r11====== | ---22--- | ......33\rField widths and float precision are handled in a way similar to placeholders. The following two print statements are equivalent:\nprint('This is my ten-character, two-decimal number:%10.2f' %13.579)\rprint('This is my ten-character, two-decimal number:{0:10.2f}'.format(13.579))\rThis is my ten-character, two-decimal number: 13.58\rThis is my ten-character, two-decimal number: 13.58\rNote that there are 5 spaces following the colon, and 5 characters taken up by 13.58, for a total of ten characters.\nFormatted String Literals (f-strings) Introduced in Python 3.6, f-strings offer several benefits over the older .format() string method described above. For one, you can bring outside variables immediately into to the string rather than pass them as arguments through .format(var).\nname = 'Fred'\rprint(f\u0026quot;He said his name is {name}.\u0026quot;)\rHe said his name is Fred.\rPass !r to get the string representation:\nprint(f\u0026quot;He said his name is {name!r}\u0026quot;)\rHe said his name is 'Fred'\rFloat formatting follows \u0026quot;result: {value:{width}.{precision}}\u0026quot; Where with the .format() method you might see {value:10.4f}, with f-strings this can become {value:{10}.{6}}\nnum = 23.45678\rprint(\u0026quot;My 10 character, four decimal number is:{0:10.4f}\u0026quot;.format(num))\rprint(f\u0026quot;My 10 character, four decimal number is:{num:{10}.{3}}\u0026quot;)\rMy 10 character, four decimal number is: 23.4568\rMy 10 character, four decimal number is: 23.5\rNote that with f-strings, precision refers to the total number of digits, not just those following the decimal. This fits more closely with scientific notation and statistical analysis. Unfortunately, f-strings do not pad to the right of the decimal, even if precision allows it:\nnum = 23.45\rprint(\u0026quot;My 10 character, four decimal number is:{0:10.4f}\u0026quot;.format(num))\rprint(f\u0026quot;My 10 character, four decimal number is:{num:{10}.{6}}\u0026quot;)\rMy 10 character, four decimal number is: 23.4500\rMy 10 character, four decimal number is: 23.45\rIf this becomes important, you can always use .format() method syntax inside an f-string:\nnum = 23.45\rprint(\u0026quot;My 10 character, four decimal number is:{0:10.4f}\u0026quot;.format(num))\rprint(f\u0026quot;My 10 character, four decimal number is:{num:10.4f}\u0026quot;)\rMy 10 character, four decimal number is: 23.4500\rMy 10 character, four decimal number is: 23.4500\r","date":1676160000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1676160000,"objectID":"605229bdd8e5122c285c37f1b696b734","permalink":"https://pudasainimohan.com.np/post/string_formating/","publishdate":"2023-02-12T00:00:00Z","relpermalink":"/post/string_formating/","section":"post","summary":"Comprehensive overview of the three methods for string formatting in Python - Placeholder Method using % character, .format() Method, and f-strings. The article explores each of these methods with examples, and provides a detailed explanation of the differences between the %s and %r placeholders, as well as techniques for controlling the display of floating-point numbers in string formatting","tags":["Python Basics","String","Data Science"],"title":"String Formatting in Python","type":"post"},{"authors":["Mohan Kumar Pudasaini"],"categories":["Python"],"content":"In Python, strings are a vital data type that play a significant role in storing text information, such as names, addresses, and other textual data. Strings are viewed as a series of characters, each with a unique position or index in the sequence. This allows us to easily access individual characters within a string using indexing.\nFor instance, the string \u0026ldquo;Nepal\u0026rdquo; is considered a sequence of five letters (\u0026lsquo;N\u0026rsquo;, \u0026rsquo;e\u0026rsquo;, \u0026lsquo;p\u0026rsquo;, \u0026lsquo;a\u0026rsquo;, \u0026rsquo;l\u0026rsquo;) arranged in a specific order. With the help of indexing, we can extract individual letters from the string, such as the first letter \u0026lsquo;N\u0026rsquo; or the last letter \u0026rsquo;l\u0026rsquo;. This property of strings as sequences in Python makes them extremely versatile and useful for a variety of applications, including text processing and manipulation.\nIn this article, we will explore the creation, indexing, formatting, and various properties of strings in Python.\nCreating a String To create a string in Python you need to use either single quotes or double quotes. For example:\n'Nepal'\r'Nepal'\rWe can also use double quote\r\u0026quot;Buddha was born in Nepal\u0026quot;\r'Buddha was born in Nepal'\r' I'm passionate about data science'\rFile \u0026quot;\u0026lt;ipython-input-4-da9a34b3dc31\u0026gt;\u0026quot;, line 2\r' I'm passionate about data science'\r^\rSyntaxError: invalid syntax\r** error?**\nThe reason for the error above is because the single quote in I\u0026rsquo;m stopped the string. You can use combinations of double and single quotes to get the complete statement or use backslash(\\).\n\u0026quot;I'm passionate about data science\u0026quot;\r'I\\'m passionate about data science'\r'I'm passionate about data science'\rPrinting a String The example above only displays one output despite having two lines of input. Is your assumption that it\u0026rsquo;s due to the identical sentences ? It\u0026rsquo;s important to note that simply entering a string in a Jupyter notebook cell will result in automatic output, but to display multiple strings properly, the print function should be utilized. to show both line we can do as follow\nprint(\u0026quot;I'm passionate about data science\u0026quot;)\rprint('I\\'m passionate about data science')\rI'm passionate about data science\rI'm passionate about data science\rWe can use a print statement to print a string. here are some examples and their outputs:\nprint('Hello World 1')\rprint('Hello World 2')\rprint('Use \\n to print a new line')\rprint('\\n')\rprint('See what I mean?')\rHello World 1\rHello World 2\rUse to print a new line\rSee what I mean?\rString length In Python, you can find the length of a string using the len function. The len function returns the number of characters in a string. Here\u0026rsquo;s an example:\nlen('Hello World')\r11\rString Indexing string indexing is a process of accessing individual characters in a string using an index. The index of a character in a string starts from 0, and the last character in a string has an index of len(string) - 1. You can access a character at a specific index in a string using square brackets []. Let\u0026rsquo;s learn how this works.\nAssign s as string and print it\rs = 'Hello World'\rPrint(s)\r'Hello World'\rShow first element (in this case a letter)\rs[0]\r'H'\rs[1]\r'e'\rWe can use a : to perform slicing which grabs everything up to a designated point. For example:\ns[1:]\r'ello World'\rhere, [1:] is a slice of the string that starts from the second character (index 1) and goes to the end of the string.\ns[:3]\r'Hel'\rIn this case, we tell Python to extract the characters from the first position (index 0) until the fourth position (index 3), but not including the character at the fourth position. This concept of \u0026ldquo;up to but not including\u0026rdquo; is a common occurrence in Python, and can be seen frequently in various statements and contexts. We can also use negative indexing to go backwards.\ns[-1]\r'd'\rWe can extract elements from a sequence by using index and slice notation, where the step size can be specified. To do so, we can use two colons followed by a number that represents the frequency of extraction. For example, to retrieve all elements with a step size of 2, we can use the following syntax.\ns[::2]\r'HloWrd'\rSimilary, We can use this to print a string backwards too\ns[::-1]\r'dlroW olleH'\rString Properties It\u0026rsquo;s important to note that strings have an important property known as immutability. This means that once a string is created, the elements within it can not be changed or replaced. For example:\ns\r'Hello World'\rLet\u0026rsquo;s try to change the first letter to \u0026lsquo;x\u0026rsquo;\ns[0] = 'x'\r---------------------------------------------------------------------------\rTypeError Traceback (most recent call last)\r\u0026lt;ipython-input-26-976942677f11\u0026gt; in \u0026lt;module\u0026gt;()\r1 # Let's try to change the first letter to 'x'\r----\u0026gt; 2 s[0] = 'x'\rTypeError: 'str' object does not support item assignment\rNotice how the error tells us directly what we can\u0026rsquo;t do, change the item assignment!\nSomething we can do is concatenate strings!\ns= s + ' concatenate me!'\rprint(s)\r'Hello World concatenate me!'\rwe can use the multiplication operator to achieve repetition of elements.\nletter = 'z'\rletter*10\r'zzzzzzzzzz'\rBasic Built-in String methods Python objects often have pre-defined methods that come as part of the object. These methods, which are essentially functions within the object, can perform various operations on the object itself. To access these methods, we use the dot notation, followed by the method name. The general syntax for using methods is as follows:\nobject.method(arguments)\nWhere the arguments are optional parameters that can be passed to the method. If some parts of this explanation are not clear at this moment, don\u0026rsquo;t worry, we will delve into it further when we start creating our own objects and functions.\nBelow are a few examples of built-in methods for strings:\ns\r'Hello World concatenate me!'\rTo change to uppercase:\ns.upper()\r'HELLO WORLD CONCATENATE ME!'\rTo change to lowercase:\ns.lower()\r'hello world concatenate me!'\rSplit a string by blank space (this is the default):\ns.split()\r['Hello', 'World', 'concatenate', 'me!']\rSplit by a specific element (doesn\u0026rsquo;t include the element that was split on)\ns.split('W')\r['Hello ', 'orld concatenate me!']\rThere are many more methods than the ones covered here. we will cover it on future posts\nPrint Formatting We can use the .format() method to add formatted objects to printed string statements.\nThe easiest way to show this is through an example:\n'Insert another string with curly brackets: {}'.format('The inserted string')\r'Insert another string with curly brackets: The inserted string'\rWe will discuss other options for print formatting in future articles.\n","date":1676073600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1676073600,"objectID":"04a1e8507fee6b6fc81d3361118d0d40","permalink":"https://pudasainimohan.com.np/post/string_basic/","publishdate":"2023-02-11T00:00:00Z","relpermalink":"/post/string_basic/","section":"post","summary":" Basics of strings in Python, including how to create strings, use indexing and slicing to access individual characters, and understand the immutability property of strings","tags":["Python Basics","String","Data Science"],"title":"An Introduction to Strings in Python","type":"post"},{"authors":["Mohan Kumar Pudasaini"],"categories":["Python"],"content":"Python offers a range of numeric data types, including integers, floating-point numbers, and complex numbers. In this article, our main focus will be on integers and floating-point numbers. Integers are whole numbers that can be either positive or negative, for instance 7 or -10. Floating-point numbers, on the other hand, have decimal components or use exponential (e) notation, such as 2.0, -2.1, or 4e2.\nHere is a table of the two main types we will spend most of our time working with some examples:\nExamples Number \"Type\"\r1,2,-5,1000\rIntegers 1.2,-0.5,2e2,3E2 Floating-point numbers Now let\u0026rsquo;s start with some basic arithmetic.\nBasic Arithmetic Python provides several built-in arithmetic operators for basic calculations, including: addition (+), subtraction (-), multiplication (*), division (/), and modulo division (%). Additionally, it provides the exponentiation operator (**) which computes the power of a number and the floor division operator (//) which returns the integer quotient of division. Here are some examples of each of these operators:\nAddition\r7+4\r11\rSubtraction\r7-4\r3\rMultiplication\r7*4\r28\rDivision\r7/4\r1.75\rFloor Division:\r7//4\rUnexpected result from 7 divided by 4 equals 1, not 1.75. This is because of the use of floor division (// operator). It returns the integer result by truncating the decimal.\nWhat if we just need the remainder?\n7%4\r3\r4 goes into 7 once, with a remainder of 3. The % operator returns the remainder after division. let\u0026rsquo;s continue with some other operators.\nExponential\r2**3\rThe expression 2**3 in Python calculates 2 raised to the power of 3, which equals 8. The ** operator is used for exponentiation in Python. similary we can use power 0.05 to calculate the square root.\n4**0.5\r2.0\rVariable Assignments With a basic understanding of numbers in Python, let\u0026rsquo;s move on to assigning labels to values through the use of variables. To create a variable, you simply use a single equals sign (=) followed by the value you want to assign. Let\u0026rsquo;s explore a few examples to illustrate this process.\na = 10\rWe have created an object with a value of 10. If we reference the object, designated as a, within our Python script, Python will recognize it as the number 10\na+a\r20\rWhat happens on reassignment? Will Python let us write it over?\na = 15\rlet\u0026rsquo;s check\na\r15\rPython lets you reassign variables with a reference to the same object.\na=a+15\ra\r30\rThere\u0026rsquo;s actually a shortcut for this. Python lets you add, subtract, multiply and divide numbers with reassignment using +=, -=, *=, and /=.\na += 15\ra\r45\rYes! Python allows you to write over assigned variable names. We can also use the variables themselves when doing the reassignment.\nWhen creating variables or objects in Python, certain guidelines must be followed to ensure their names are valid. This includes:\nStarting the name with a letter, not a number. Utilizing underscores(_) instead of spaces. Avoiding special characters like \u0026lsquo;\u0026rsquo;, \u0026ldquo;, \u0026lt;, \u0026gt;, /, ?, |, , (, ), !, @, #, $, %, ^, \u0026amp;, *, ~, -, and +. Keeping names in lowercase, as recommended by PEP8. Steering clear of single letter names such as l, O, and I which can be easily misconstrued. Refraining from using words that hold special meaning in Python, for instance, \u0026ldquo;list\u0026rdquo; or \u0026ldquo;str\u0026rdquo;. Assigning values to variables in Python can greatly aid in organizing and tracking the data in your code. Here\u0026rsquo;s an example\nmy_salary = 50000\rtax_rate = 0.13\rtotal_tax = my_income*tax_rate\rBy accessing the total_tax variable, we can easily calculate the tax amount:\ntotal_tax\r6500.0\r","date":1675987200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1675987200,"objectID":"02bbd20a4e06092027feb27c7b31eb1a","permalink":"https://pudasainimohan.com.np/post/arithmetic_python/","publishdate":"2023-02-10T00:00:00Z","relpermalink":"/post/arithmetic_python/","section":"post","summary":" Featuring an Analysis of Types of Numbers, Basic Arithmetic Procedures, the Distinction between Classic and Floor Division, and Object Allocation.","tags":["Python Basics","Airthmetic Operators","Data Science"],"title":"Numbers and airthmetic Operators in Python","type":"post"},{"authors":["Mohan Kumar Pudasaini"],"categories":["Pyhton"],"content":"Once you have installed Jupyter Notebook, you can start it by opening your terminal or command prompt and typing \u0026ldquo;jupyter notebook\u0026rdquo;. This will launch Jupyter Notebook in your default web browser and open a new tab with the Jupyter Notebook dashboard.\nIf you haven\u0026rsquo;t installed Jupyter Notebook, follow this link to get started: Get Started with Jupyter notebook\nTo open a new Jupyter Notebook in Jupyter Notebook dashboard, you can click on the \u0026ldquo;New\u0026rdquo; button on the right side of the screen, which will open a new tab in your web browser with a new Jupyter Notebook. From there, you can start writing and running your code. Now we are ready to write, note, and execute code. Let\u0026rsquo;s begin using Jupyter Notebook and its shortcut keys to perform high-level capabilities\nCommand vs. Edit Modes In Jupyter Notebook, there are two modes: command mode and edit mode.\nCommand mode -Allows you to perform actions at the notebook level. Indicated by a grey cell border with a blue left margin. Edit mode -Allows you to type within a cell. Indicated by a green cell border with a green left margin. If you\u0026rsquo;re in command mode, press Enter to enter edit mode. If you\u0026rsquo;re in edit mode, press Esc to enter command mode.\nCommand Mode In Command mode, you can perform various actions on cells, such as creating new cells, deleting cells, copying cells, cutting cells, and pasting cells. You can also move cells up and down, merge cells, split cells, and navigate to different cells. here are some examples\nRun current cell and selecct next cell: Shift + Enter Run the current cell and select same cell: Ctrl+ Enter Run the current cell and insert new cell below: Alt+ Enter Save notebook: Ctrl + S Insert a new cell above the current cell : A Insert a new cell below the current cell : B Copy and paste : Copy with C and then paste it with V Deleting cell : D D (i.e., hit the D button twice) Merge Cell: First select the cells to merge by pressing Shift + ‚¨Ü or ‚¨á then Merge using Shift + M Undo : Esc + Z View all keyboard shortcut: H (in Command mode) To write code, you need to switch the cell to code mode. For comments and notes, you need to switch the cell to markdown mode. By default, new cells are in code mode, but you can use the following shortcut keys to switch between markdown and code.\nY change the cell type to Code modes.\nM change the cell type to Markdown modes.\nEdit Mode In Edit mode, you can edit and modify the content within a cell. This mode allows you to enter and edit text, code, and markdown in the selected cell. Some common actions you can perform in Edit mode include typing, formatting text, executing code, and creating new lines. To enter Edit mode, simply click on a cell or press \u0026lsquo;Enter\u0026rsquo; key when the cell is selected. To exit Edit mode, press \u0026lsquo;Esc\u0026rsquo; or click outside the cell. here are some functionalities and shortcut keys for edit modes.\nComment in the text Ctrl + / quick access to all the commands in Jupyter Notebooks: Ctrl + Shift + P Insert Image ![caption](picture name and location)\n![screenshot](mohan1.jpg)- This command imports the picture named \u0026lsquo;mohan1.jpg\u0026rsquo; located in the same folder as the Jupyter notebook.\nMarkdown Jupyter Notebook\u0026rsquo;s markdown cells provide a way to write formatted text, headings, bullet points, links, images, and more. Markdown is a simple markup language that is easily readable and provides a lightweight way to add formatting to text.here are some examples:\nHeading In Markdown, headings can be created by using the hash symbol (#) followed by a space and the text of the heading. The number of hashes used determines the level of the heading, with one hash being the largest heading (H1) and six hashes being the smallest (H6). For example\ncode:\n# Heading 1 ## Heading 2 ### Heading 3\r#### Heading 4\r##### Heading 5\r###### Heading 6\routput:\nHeading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 Emphasis bold, italic, and strikethrough are formatting options in Markdown that can be used to add visual interest and clarity to text. here are some examples:\nCode:\n*Italic* _Italic_\r**B0ld** __Bold__\r~~Strikethrough.~~\rOutput:\nItalic Italic\nBold Bold\nStrikethrough.\nLists Lists are a great way to organize and present information in Markdown. here is some examples how we can make list in ordered and unordered way in markdown.\nUnordered list can be creat using *,+,- for example: Code:\n+ Item 1\r- Item 2\r* Item 3\rOutput:\nItem 1 Item 2 Item 3 Ordered list can be creat using numerical order: Code\n1. Item 1\r2. Item 2\r3. Item 3\rOutput\nItem 1 Item 2 Item 3 Other Examples of list and sublist Code:\n1. Today's\r2. date is:\r* Monday\r* 16th January 1991\r- hello! how are\r- how are you?\r- Hope doing well Output:\nToday\u0026rsquo;s date is: Monday 16th January 1991 hello! how are how are you? Hope doing well Code\n- [ ] Item A\r- [x] Item B\r- [x] Item C\rOutput\nItem A Item B Item C ","date":1675900800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1675900800,"objectID":"3c12b8ae9f357118314a1e76250c5d48","permalink":"https://pudasainimohan.com.np/post/jupyter_start/","publishdate":"2023-02-09T00:00:00Z","relpermalink":"/post/jupyter_start/","section":"post","summary":"The effective use of Jupyter Notebook enhances coding efficiency and the article highlights various shortcut keys","tags":["Python Basics","Jupyter notebook","Data Science"],"title":"Work on Jupyter Notebook:Shortcut Keys","type":"post"},{"authors":["Mohan Kumar Pudasaini"],"categories":["Python"],"content":"Jupyter Notebook is a free, open-source web-based platform that enables users to create and share documents that combine live code, mathematical equations, graphics, and narrative text. It has become a popular tool in the data science, machine learning, and scientific computing domains for conducting interactive computing and data analysis. The platform is user-friendly and enables collaboration through sharing and reproducibility. It is most popular platforom for python users.\nInstalling Jupyter Notebook is a simple process that can be done in different ways, depending on your operating system and the resources available on your computer. In this article, we will look at the different ways to install Jupyter Notebook on your computer.\nInstalling Jupyter Notebook using Anaconda Anaconda is a distribution of the Python and R programming languages that comes with a comprehensive set of tools for data science and machine learning. To install Jupyter Notebook using Anaconda, follow these steps:\n1. Download and install Anaconda: Visit the Anaconda website (\rhttps://www.anaconda.com/products/distribution) and download the latest version of Anaconda for your operating system. 2. Start Anaconda Navigator: After installation, you can start Anaconda Navigator from your applications menu or by typing \u0026ldquo;anaconda-navigator\u0026rdquo; in your terminal (MacOS/Linux) or Command Prompt (Windows).\n3.Launch Jupyter Notebook: In Anaconda Navigator, click on the Jupyter Notebook icon to launch the Jupyter Notebook application Installing Jupyter Notebook using pip pip is a package manager for Python that allows you to install and manage packages from the Python Package Index (PyPI). to installed the jupyter notebook first, check wether pip is installed in you system by typing \u0026lsquo;pip \u0026ndash;version\u0026rsquo; in you command prompt.. If pip is installed, you should see the version number of pip, for example, ‚Äúpip 22.3.1‚Äù.o install Jupyter Notebook using pip, follow these steps:\n1. Install Jupyter Notebook Open your terminal (MacOS/Linux) or Command Prompt (Windows) and type the following command:\npip install jupyter\rThis will install Jupyter Notebook and any dependencies required for the application to run.\n2. Launch Jupyter Notebook After the installation is complete, you can launch Jupyter Notebook by typing the following command in your terminal or Command Prompt\njupyter notebook\rThis will start the Jupyter Notebook server and open a web browser window with the Jupyter Notebook interface.\nThere are numerous additional Integrated Development Environments (IDEs) besides Jupyter Notebook that you can use to run Python.Some popular options include:\nVisual code studio PyCharm Python\u0026rsquo;s built-in IDE(IDLE) Spyder Whichever IDE you decide on, it\u0026rsquo;s essential to have a decent understanding of the Python programming language and its syntax before beginning.\nIn conclusion, Jupyter Notebook is a powerful tool for data science and machine learning, but it is not the only option for running Python. Depending on your specific needs and preferences, other IDEs such as PyCharm, IDLE, Spyder, or Visual Studio Code may be more suitable for your projects. It\u0026rsquo;s important to familiarize yourself with the different options and choose the one that works best for you.\n","date":1675814400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1675814400,"objectID":"50cfea042533ef03b592f73515b7f175","permalink":"https://pudasainimohan.com.np/post/jupyter_install/","publishdate":"2023-02-08T00:00:00Z","relpermalink":"/post/jupyter_install/","section":"post","summary":"This guide provides a step-by-step process to help you download, install and set up Jupyter notebook on your computer.","tags":["Python Basics","Jupyter notebook","Data Science"],"title":"Get Started with Jupyter notebook","type":"post"},{"authors":["Mohan Kumar Pudasaini"],"categories":["Python"],"content":"Python is a popular, high-level programming language used for web development, scientific computing, data analysis, artificial intelligence, and more. Here is a simple guide to help you install Python on your computer\nThere are two main ways to install Python: using a package manager or downloading the Python executable.\nUsing Package Manager: One option to install Python on your computer is to use a package manager or a platform-specific store. For example, on macOS, you can use the Homebrew package manager to install Python. Simply open Terminal and type \u0026ldquo;brew install python\u0026rdquo; to download and install the latest version of Python and its dependencies. On Windows, you can use the Microsoft Store to install Python. Simply search for \u0026ldquo;Python\u0026rdquo; in the Microsoft Store, select the latest version, and click the \u0026ldquo;Get\u0026rdquo; or \u0026ldquo;Install\u0026rdquo; button to download and install Python on your computer. These package managers and platform-specific stores provide a convenient and efficient way to install and manage software packages, including Python, without having to manually download and install them. Downloading the Python Executable: Another option to install Python on your computer is to download the Python executable from the official Python website. The following are the basic steps to install Python using this method:\nStep 1: Go to the Official Website and download Version\nVisit the official Python website (\rhttps://www.python.org/) and navigate to the downloads section. Choose the appropriate version of Python that you want to install on your computer. For example, if you\u0026rsquo;re a beginner, you might want to start with Python 3.x.x. Once you\u0026rsquo;ve selected the appropriate version of Python, click the download button to start the download process. Make sure to save the installation file to a location where you can easily find it.\nStep 2: Click Install and Make Sure Add to Path is Checked\nOnce the download is complete, double-click the installation file to start the installation process. Follow the on-screen instructions to install Python on your computer. Make sure to check the option \u0026ldquo;Add to Path\u0026rdquo; when prompted during the installation process. This will ensure that Python is added to your system PATH, making it easier to run Python from the Command Prompt or Terminal.\nStep 3: Check Whether Python is Installed Using CMD\nOpen the Command Prompt or Terminal and type \u0026ldquo;python\u0026rdquo; or \u0026ldquo;python \u0026ndash;version\u0026rdquo; to check if Python has been installed successfully. If Python has been installed successfully, you should see a prompt that looks like this: \u0026ldquo;Python 3.11.1\u0026rdquo;. This means that you can start using Python in the Command Prompt or Terminal. Step 5: Make Sure Pip is Installed\nPip is a package management system used to install and manage Python packages. It is included with Python 3.4 and later versions. To check if pip is installed, open the Command Prompt or Terminal and type \u0026ldquo;pip \u0026ndash;version\u0026rdquo;. If pip is installed, you should see the version number of pip, for example, \u0026ldquo;pip 22.3.1\u0026rdquo;. That\u0026rsquo;s it! You\u0026rsquo;ve successfully installed Python on your computer. You\u0026rsquo;re now ready to start exploring the world of Python programming!\n","date":1675728000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1675728000,"objectID":"8f8da4a8f52316a2f2f6ba98ed197d8b","permalink":"https://pudasainimohan.com.np/post/python_install/","publishdate":"2023-02-07T00:00:00Z","relpermalink":"/post/python_install/","section":"post","summary":"This guide provides a step-by-step process to help you download, install and set up Python on your computer.","tags":["Python Basics","Data Science"],"title":"Get Started with Python: An Installation Guide","type":"post"},{"authors":["Mohan Kumar Pudasaini"],"categories":["Python"],"content":"Python is a high-level, open-source programming language with a wide range of applications, including web development, data analysis, machine learning, and deep learning. It was created in 1991 by Guido van Rossum and is maintained by the Python Software Foundation. Python is considered an essential tool for data scientists and engineers due to its simplicity, versatility, and large community, as well as its numerous libraries and tools that make it ideal for data science tasks. The language is easy to read and write, highly extensible, and has a wealth of resources available, making it a great choice for both beginners and experienced programmers alike. This article will explore the many advantages that make Python an essential language in data science.\nEasy to Learn: Python is widely recognized for its simple and intuitive syntax, which makes it a great language for beginners to learn. The syntax of Python is designed in a way that it is easy to understand and follow, even for those who are new to programming. It does not require a deep understanding of computer science concepts to get started, which allows individuals to focus on learning the basics of programming without getting bogged down by complex syntax or concepts. This simplicity and ease of use is one of the reasons why Python is a popular choice for introductory programming courses.\nVersatile: Python is a versatile programming language that is used in a wide range of fields, including web development, scientific computing, artificial intelligence, and more. This versatility makes Python a valuable language for those who are looking to work in different areas of computer science or explore various fields. The large library of modules and frameworks available in Python, as well as its ability to interact with other languages, makes it a versatile choice for many different types of projects.\nHigh Demand: Python is in high demand in the job market, particularly in fields such as data science, machine learning, and artificial intelligence. Many companies are looking for individuals who are skilled in Python, making it a valuable language to know for those who are looking to advance their careers. The demand for Python is expected to continue to grow, as more and more companies adopt data science and artificial intelligence technologies.\nLarge Community: Python has a large and active community of users, developers, and enthusiasts who are constantly creating and sharing new tools, libraries, and resources. This community provides a wealth of resources for learning and troubleshooting, making it easy for beginners to get help and support when needed. The community also provides opportunities for collaboration and networking, making it a great place for individuals to grow their skills and build their careers.\nData Analysis: Python is equipped with a variety of libraries and tools that make it easy to perform data analysis tasks, such as data cleaning, transforming, and aggregating. Some of the popular libraries for data analysis in Python include NumPy, Pandas, and Matplotlib. These libraries allow data scientists to perform complex data analysis tasks with ease, making it a great choice for data science projects.\nData Visualization: Data visualization is an important aspect of data science, and Python offers a range of libraries for creating high-quality visualizations. Some of the popular libraries for data visualization in Python include Matplotlib, Seaborn, and Plotly. These libraries make it easy to create visualizations that help communicate insights and make data more accessible to a wider audience.\nMachine Learning: Python has a rich ecosystem of libraries for machine learning, which makes it a popular choice for building machine learning models. Some of the popular libraries for machine learning in Python include scikit-learn, TensorFlow, and PyTorch. These libraries provide a range of tools for training and evaluating machine learning models, as well as for tasks such as feature selection, hyperparameter tuning, and model deployment.\nDeep Learning: Deep learning is a subfield of machine learning that deals with building models that can learn and make decisions based on large amounts of data. Python has a range of libraries for deep learning, including TensorFlow, PyTorch, and Keras, which make it easy to build and train deep learning models. These libraries provide a range of tools and resources for building complex deep learning models, making Python a great choice for deep learning projects. In conclusion, Python plays a crucial role in data science and is an essential tool for data scientists and engineers. Its user-friendly syntax, versatility, and extensive libraries make it a popular choice for tasks such as data analysis, data visualization, machine learning, and deep learning.\n","date":1675641600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1675641600,"objectID":"52b56e9c872f67c05d73ff91a8f7bb5a","permalink":"https://pudasainimohan.com.np/post/ml_python_intro/","publishdate":"2023-02-06T00:00:00Z","relpermalink":"/post/ml_python_intro/","section":"post","summary":"The Advantages of Python in Data Science: Easy to Learn, Versatile, High Demand, Large Community, and Equipped with Libraries for Data Analysis, Visualization, Machine Learning, and Deep Learning.","tags":["Python Basics","Data Science"],"title":"Importance of Python in Data Science: A  Overview of its Role and Advantages","type":"post"},{"authors":["Mohan Kumar Pudasaini"],"categories":null,"content":"Hello everyone,\nWelcome to my personal portfolio website! This site will be dedicated to sharing my skills and knowledge related to SQL, R , Python and other data science related topics. I am passionate about these fields and aim to share my expertise with others in the hopes of supporting and inspiring others.\nI will be regularly posting updates on this site, so stay tuned for new content. Your feedback is highly valued and appreciated, so please feel free to reach out to me with any questions or comments. You can find my contact information on the bio page above.\nI hope you enjoy exploring this site and learning with me. Let\u0026rsquo;s take this journey together!\nBest regards, Mohan\n","date":1675555200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1675555200,"objectID":"8411dceeeef66a91940fc09446c2dd63","permalink":"https://pudasainimohan.com.np/post/fist_post/","publishdate":"2023-02-05T00:00:00Z","relpermalink":"/post/fist_post/","section":"post","summary":"Welcome üëã .","tags":null,"title":"Welcome to my website","type":"post"},{"authors":null,"categories":null,"content":"I am honored to have participated in the WCO Pre-Accreditation Workshop for Technical and Operational Advisers in Risk Management, held in Bangkok, Thailand, from 21 to 25 November 2022, organized by the World Customs Organization (WCO) in cooperation with the Royal Thai Customs and the WCO Regional Office for Capacity Building (ROCB) Asia/Pacific.\nThis workshop was a crucial step in the WCO‚Äôs accreditation process, designed to assess and strengthen both the technical knowledge and facilitation capabilities of selected experts from the Asia-Pacific region. Throughout the week, we engaged in peer learning, individual presentations, and technical exercises focused on identifying, analyzing, and mitigating risk in Customs operations.\nIt was an inspiring and enriching experience to learn alongside talented professionals dedicated to improving customs risk management globally. I am grateful to the WCO Secretariat for this oppertunity.\nLooking forward to continuing this journey of knowledge sharing and contributing to future WCO capacity-building initiatives in the field of risk management.\nNews in WCO website: WCO Risk Management Pre-Accreditation Workshop for the Asia Pacific region in Bangkok\nNews in WCO Asia and the pacific website: WCO Customs Risk Management Pre-Accreditation Workshop for the Asia/Pacific Region in Bangkok ","date":1669021200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669021200,"objectID":"51ab6496b8af4f1140c22509896a0c41","permalink":"https://pudasainimohan.com.np/talk/2022_rm_accreditation/","publishdate":"2022-11-30T00:00:00Z","relpermalink":"/talk/2022_rm_accreditation/","section":"talk","summary":"I am excited to share my experience attending the World Customs Organization (WCO) Pre-Accreditation Workshop on Risk Management , held in Thailand from November 21 to  25, 2022. This workshop was a significant milestone in my professional journey, as it focused on evaluating various aspects of technical expertise and facilitation skills.","tags":[],"title":"Participated WCO Pre-Accreditation workshop on Risk Managemet","type":"talk"},{"authors":null,"categories":null,"content":"Mohan Pudasaini organized a 5-day virtual workshop titled \u0026ldquo;Writing Reproducible Research Papers with R\u0026rdquo; from August 25th to 29th, 2022. The workshop was designed to help students, researchers, data analysts, and teaching faculties write scientific journal papers that are reproducible and can be easily shared with others.\nThe workshop had 45 participants from different parts of the world, who attended virtually through Zoom. The participants learned how to create dynamic documents using R markdown and how to format documents in predefined templates and their own templates in R markdown. They were also taught how to arrange titles, subtitles, and dates, manage text format and style, manage references and citations, develop tables, and more using R markdown.\nThe workshop was organized by Mohan Pudasaini in his personal initiative to help researchers and students enhance their skills in writing reproducible research papers. The participants were required to have basic knowledge of R and R studio to attend the workshop.\nThe first four days of the workshop were dedicated to teaching the participants about the different components of R markdown and how to use them. The final day of the workshop was focused on practical application, where the participants were given a sample research paper to format and finalize using the skills they had learned over the previous days. ","date":1661457600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661457600,"objectID":"768949ed49181fa52ba917eb40f4b212","permalink":"https://pudasainimohan.com.np/talk/2022_rmarkdown/","publishdate":"2023-03-27T00:00:00Z","relpermalink":"/talk/2022_rmarkdown/","section":"talk","summary":"Mohan Pudasaini organized a 5-day virtual workshop on \"Writing Reproducible Research Papers with R\" focusing on the use of R and R markdown for 45 participants, where they learned how to write reproducible research papers, manage text format and style, develop tables, and more.","tags":[],"title":"Workshop on Writing Reproducible Research Papers with R","type":"talk"},{"authors":null,"categories":null,"content":"The Department of Customs (DOC), Nepal, with support from the Asian Development Bank (ADB), organized a Risk Management Workshop for customs officials of Nepal from 8 to 10 August 2022 in Pokhara, Nepal. The workshop aimed to lay the foundation for effective risk management in customs, develop risk registers and risk profiles for use in the ASYCUDA World Selectivity Module, and train customs officials in risk management to develop them into a pool of experts.\nFifteen customs officials from DOC and field offices participated in the training program, which covered a range of sessions. These included identifying gaps and recommended measures, sharing experiences on the development of risk management in Nepal, reviewing the risk management framework and standard operating procedures to pave the way for their effective implementation, and exploring the use of machine learning in risk management. One notable session was a presentation on \u0026ldquo;Machine Learning Approach for Customs Fraud Detection\u0026rdquo; by Mr. Mohan Pudasaini.\nThe workshop produced outputs such as a risk register, risk profile, and a work plan to strengthen risk management in customs. The training program was inaugurated in virtual mode by Mr. Yagya Raj Koirala and Mr. Ananda Kafle, Deputy Director Generals of DOC, and they also attended the concluding day program to provide guidance for the way forward. ","date":1659952800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1659952800,"objectID":"e6b9abbac614da610871feddf4a204e6","permalink":"https://pudasainimohan.com.np/talk/2022_pokhara/","publishdate":"2023-02-14T00:00:00Z","relpermalink":"/talk/2022_pokhara/","section":"talk","summary":"The Department of Customs Nepal, organized a Risk Management Workshop for customs officials in August 2022. The workshop aimed to build a foundation for effective risk management, develop risk registers and profiles, and train officials in risk management. Sessions covered identifying gaps and recommended measures, reviewing frameworks and procedures, and exploring the use of machine learning in risk management.","tags":[],"title":"Department of Customs organized Workshop on Risk Management at Pokhara","type":"talk"},{"authors":null,"categories":null,"content":"Mohan Pudasaini organized a 3-week virtual workshop titled \u0026ldquo;Research Data Analysis using R\u0026rdquo; from June 25th to July 15th, 2022. The workshop covered various topics such as Introduction to R, Data Cleaning, Data Visualization, Statistical Tests, Regression Analysis, ANOVA, ANCOVA, and Non-Parametric Tests. The workshop was designed to help participants, including students, researchers, data analysts, and teaching faculties, write reproducible scientific journal papers that can be easily shared with others. The workshop had 70 participants from different parts of the world who attended virtually through Zoom.\nThe workshop was organized by Mohan Pudasaini in his personal initiative and was spread over 21 days, with each day covering a different topic. Topics covered included data types and structure, packages in R, importing various file types, hypothesis testing, regression analysis, ANOVA, ANCOVA, non-parametric tests, and more. The workshop aimed to provide participants with a comprehensive understanding of data analysis using R and help them gain practical experience by working on examples and exercises. Overall, the workshop was a great success and helped participants enhance their skills in data analysis using R. ","date":1656187200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656187200,"objectID":"644138b713bc64eaa67c29757e4705c5","permalink":"https://pudasainimohan.com.np/talk/2022_research_analysis/","publishdate":"2023-02-14T00:00:00Z","relpermalink":"/talk/2022_research_analysis/","section":"talk","summary":"Mohan Pudasaini organized a 3-week virtual workshop titled \"Research Data Analysis Using R\" to help individuals write scientific journal papers that can be easily shared and reproduced. The workshop covered various topics related to research data analysis, including R introduction, data cleaning, data visualization, statistical tests, regression analysis, and ANOVA. 70 participants attended the workshop virtually through Zoom from different regions of the world.","tags":[],"title":"Virtual Workshop on Research Data Analysis Using R","type":"talk"},{"authors":null,"categories":null,"content":"On June 23, 2022, Hetauda School of Management, Hetauda organized a one-day workshop on \u0026ldquo;R for data analysis\u0026rdquo; with the aim to equip lecturers from Hetauda School of Management, Hetauda Campus, and Makawanpur Multiple Campus with the skills needed to use R for data manipulation, cleaning, and modeling.\nThe workshop was attended by more than 30 lecturers, and Mr. Mohan Pudasaini was the trainer for the workshop. The focus of the workshop was on the practical application of R in data analysis and the lecturers were introduced to the various tools and techniques that can be used to effectively manipulate, clean and model data using R.\nThe workshop was an excellent opportunity for the lecturers to learn and enhance their skills in data analysis, and it is expected to have a positive impact on their teaching and research in the future. ","date":1655978400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655978400,"objectID":"1e09ed6b9f47ab945861f35fb555ab89","permalink":"https://pudasainimohan.com.np/talk/2022_hetauda/","publishdate":"2023-02-14T00:00:00Z","relpermalink":"/talk/2022_hetauda/","section":"talk","summary":"Hetauda School of Management, Hetauda organized a one-day workshop on \"R for data analysis\" on June 23, 2022. Over 30 lecturers from Hetauda School of Management, Hetauda Campus and Makawanpur Multiple Campus attended the workshop. The workshop aimed to equip the attendees with skills to manipulate, clean and model data using R.","tags":[],"title":"R for Data Analysis Workshop at Hetauda School of Management","type":"talk"},{"authors":null,"categories":null,"content":"The Health Directorate of Sudurpashim Province organized a five-day workshop in Dhangadhi, Nepal, focused on data analysis for health planning personnel. The workshop was attended by more than 20 participants from various district health offices, who were actively involved in data management and analysis. The workshop was facilitated by Mr. Mohan Pudasaini.\nDuring the workshop, participants were trained on various aspects of data analysis using R, including data cleaning and processing, statistical analysis, and data visualization. The data used for analysis was obtained from the Health Management Information System (HMIS) in Nepal, and participants were shown how to analyze the data from different perspectives related to health.\nParticipants learned how to use data to identify trends and patterns, make data-driven decisions, and communicate insights effectively. The workshop provided valuable skills and knowledge to improve the quality of their data analysis and reporting, which can ultimately enhance the decision-making process in the field of health.\n","date":1641204000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641204000,"objectID":"1f0f37f9e20ddc42ae4fadb64da8724c","permalink":"https://pudasainimohan.com.np/talk/2022_dhangadhi/","publishdate":"2023-02-14T00:00:00Z","relpermalink":"/talk/2022_dhangadhi/","section":"talk","summary":"The Health Directorate of Sudurpashim Province organized a five-day data analysis workshop in Dhangadhi, Nepal. The workshop was attended by more than 20 participants from various district health offices, who were actively involved in data management and analysis. The workshop was facilitated by Mr. Mohan Pudasaini","tags":[],"title":"Data Analysis Workshop for Health Planning Personnel","type":"talk"},{"authors":null,"categories":null,"content":"The Ministry of Industry, Commerce and Supplies (MoICS) organized a five-day data analysis workshop in Kathmandu, Nepal. The workshop was attended by more than 15 participants from various sections and offices under MoICS and was facilitated by Mr. Mohan Pudasaini.\nThe main focus of the workshop was trade data analysis using R, with the aim of gaining insights and values from the data. The workshop was held from July 8, 2021, to July 13, 2021.During the workshop, participants were trained on various aspects of data analysis, including data cleaning, processing, and statistical analysis. They also learned how to visualize data and use it to identify trends and patterns.I hopes that the knowledge gained from the workshop will be applied to improve trade policies and programs in Nepal. By using data to make evidence-based decisions\nIn the closing ceremony of the workshop, Joint Secretary of MoICS Mr. Prakash Dahal emphasized the importance of data analysis in the context of trade ministry and its impact on policy making. He also wished all the participants a better future, hoping that they would apply the knowledge they had gained to their work in the ministry.\n","date":1625738400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625738400,"objectID":"d464a6008e037fddf263d0745effe5ba","permalink":"https://pudasainimohan.com.np/talk/2021_moics/","publishdate":"2023-02-14T00:00:00Z","relpermalink":"/talk/2021_moics/","section":"talk","summary":"The Ministry of Industry, Commerce and Supplies (MoICS), Nepal  organized a five-day data analysis workshop in Kathmandu. The workshop was attended by more than 15 participants from various sections and offices under MoICS and was facilitated by Mr. Mohan Pudasain","tags":[],"title":"Trade Data Analysis Workshop in Mininstry of Industries Commerce and Supplies, Nepal","type":"talk"},{"authors":null,"categories":null,"content":"The Department of Commerce, Supplies and Consumer Protection in collaboration with Consumer Forum Nepal organized a one-day Consumer Awareness Program on March 22, 2019, at Gulmi, Nepal. The objective of this program was to make consumers, business personnel, and other concerned agencies aware of their rights and responsibilities as per The Consumer Protection Act 2018.\nThe event was presided over by Mohan Pudasaini, a representative of the Department of Commerce, Supplies, and Consumer Protection. The program was attended by various stakeholders, including representatives from government bodies, industries, and consumer groups.\nThe program began with an overview of The Consumer Protection Act 2018, which is aimed at promoting and protecting the rights of consumers in Nepal. The act provides a legal framework for consumer protection and sets out the rights and obligations of both consumers and businesses.\nMr. Pudasaini emphasized the importance of consumer awareness in ensuring that consumers are able to make informed choices and protect themselves against unfair trade practices. He highlighted the various rights of consumers, such as the right to safety, right to information, right to choose, and right to be heard.\nHe also emphasized the responsibilities of businesses and industries, including providing accurate information about their products and services, ensuring the safety of their products, and addressing consumer complaints in a timely and efficient manner.\nThe program also included a panel discussion on various consumer-related issues, such as the importance of product safety, the role of consumer protection agencies, and the need for effective complaint redressal mechanisms. The panelists included representatives from various industries, consumer groups, and government bodies.\n","date":1553248800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553248800,"objectID":"4013c437c7a432edfdad47e3340a51df","permalink":"https://pudasainimohan.com.np/talk/2019_gulmi/","publishdate":"2023-03-27T00:00:00Z","relpermalink":"/talk/2019_gulmi/","section":"talk","summary":"The Consumer Awareness Program in Gulmi aimed to promote consumer rights and responsibilities under The Consumer Protection Act 2018. The program featured an overview of the act and a panel discussion on consumer-related issues. The initiative raised awareness about consumer protection in Nepal and emphasized the importance of consumer awareness","tags":[],"title":"The Consumer Awareness Program at Gulmi","type":"talk"},{"authors":[],"categories":[],"content":"","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://pudasainimohan.com.np/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":".","tags":[],"title":"Under construction","type":"slides"},{"authors":null,"categories":null,"content":" Awards Best Employee Award-2021 Description: Best Employee Award was awarded by the Department of Customs in Nepal on World Customs Day in 2021\nOrganization: Department of Customs, Nepal\nCertificate Link: View Certificate\nBest Teacher Award-2013 Description: Private and Boarding School Organization awarded the Best Teacher Award for excellence in teaching, recognizing the recipient for their achievement in securing the highest Math marks in the district in SLC exam\nOrganization: Private and Boarding School Organization (PABSON)\nCertificate Link: View Certificate\nBest Teacher Award-2012 Description: Private and Boarding School Organization awarded the Best Teacher Award for excellence in teaching, recognizing the recipient for their achievement in securing the highest Math marks in the district in SLC exam\nOrganization: Private and Boarding School Organization (PABSON)\nCertificate Link: View Certificate\nCertifications WCO accredited Expert on Data Analytics Description: World Customs Organization(WCO) Accredited Technical and Operational Advisors on Data Analytics\nOrganization: World Customs Organization(WCO)\nCertificate Link: Upon Request\nWCO accredited Expert on Risk Management Description: World Customs Organization(WCO) Accredited Technical and Operational Advisors on Risk Management\nOrganization: World Customs Organization(WCO)\nCertificate Link: Upon Request\nArtificial Intelligence in Border Management Description: Course offered by the University of Victoria (Canada) in partnership with Border in Globalization(BiG) Lab\nOrganization: University of Victoria (Canada)\nCertificate Link: View Certificate\nCertificate of Appreciation Description: Certificate of appreciation as a Trainer from Sudurpashim Province, Nepal.\nOrganization: Sudur Paschim Province Government, Nepal\nCertificate Link: View Certificate\nAuthorised Economic Operator (AEO) Validation Description: An online Course offered by WCO\nOrganization: World Customs Organization(WCO)\nCertificate Link: View Certificate\nData Analytics ‚Äì Advanced (Image Data Analysis) Description: An online Course offered by WCO\nOrganization: World Customs Organization(WCO)\nCertificate Link: View Certificate\nData Analytics ‚Äì Advanced (LITE DATE: Fraud Detection) Description: An online Course offered by WCO\nOrganization: World Customs Organization(WCO)\nCertificate Link: View Certificate\nData Analytics - Intermediate Description: An online Course offered by WCO\nOrganization: World Customs Organization(WCO)\nCertificate Link: View Certificate\nData Quality management Description: An online Course offered by WCO\nOrganization: World Customs Organization(WCO)\nCertificate Link: View Certificate\nData Analytics - Beginner Description: An online Course offered by WCO\nOrganization: World Customs Organization(WCO)\nCertificate Link: View Certificate\nThe WCO SAFE Framework of Standards Description: An online Course offered by WCO\nOrganization: World Customs Organization(WCO)\nCertificate Link: View Certificate\nRisk Management Description: An online Course offered by WCO\nOrganization: World Customs Organization(WCO)\nCertificate Link: View Certificate\nThe WTO and Trade Economics -Theory and Policy Description: An online Course offered by World Trade Organization(WTO)\nOrganization: World Trade Organization(WTO) Certificate Link: View Certificate\nTime Release Study (TRS) Description: An online Course offered by WCO\nOrganization: World Customs Organization(WCO)\nCertificate Link: View Certificate\nMachine Learning with Python Description: An online non-credit course authorized by IBM and offered through Coursera\nOrganization: IBM Certificate Link: View Certificate\nProject Management Description: Seven weeks Course funded by Colombo Plan Secretariat\nOrganization: Kothari agricultural management centre, India\nCertificate Link: View Certificate\nBasic Python Organization: HackerRank\nCertificate Link: View Certificate\nThe Role of Standards in Sustainable Supply Chains Description: A online course offered by ITC\nOrganization:International Trade Centre(ITC)\nCertificate Link: View Certificate\nHow to Analyse Trade Flows Description: Online course offered by ITC\nOrganization:International Trade Centre(ITC)\nCertificate Link: View Certificate\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"87dc5752fbd2cb9b12713537a1d6df6c","permalink":"https://pudasainimohan.com.np/certificates/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/certificates/","section":"","summary":"Awards Best Employee Award-2021 Description: Best Employee Award was awarded by the Department of Customs in Nepal on World Customs Day in 2021\nOrganization: Department of Customs, Nepal\nCertificate Link: View Certificate","tags":null,"title":"Awards and certifications","type":"page"}]